{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BBC News Pipeline: Production-Grade Data Ingestion &amp; ETL","text":""},{"location":"#overview","title":"Overview","text":"<p>This repository hosts a production-grade data engineering pipeline designed to scrape BBC News (live and archived articles), transform the data into analytics-ready formats, and store it for downstream consumption. The pipeline is containerized, runs seamlessly on Kubernetes, and incorporates robust observability and reliable messaging patterns.</p> <p>Key features include:</p> <ul> <li>Reliable \u2013 Durable queues, retry mechanisms, and Dead Letter Queue (DLQ) support for fault-tolerant message delivery.</li> <li>Scalable \u2013 Horizontally scalable producers and consumers orchestrated via Kubernetes.</li> <li>Observable \u2013 Centralized metrics, logs, and dashboards using Prometheus, Grafana, and Loki.</li> <li>Portable \u2013 Fully containerized components with infrastructure-as-code (Terraform &amp; Helm) for cloud portability.</li> <li>Reproducible \u2013 Deterministic ETL pipelines with versioned data and modular architecture.</li> </ul> <p>This project demonstrates the design of a real-world end-to-end data engineering system that balances scalability, reliability, and maintainability \u2013 perfect for showcasing as a portfolio-grade project.</p>"},{"location":"#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Producers: Scrape live &amp; archived BBC News articles using Python &amp; BeautifulSoup.</li> <li>Message Queue: RabbitMQ for decoupled communication and reliable message handling.</li> <li>Consumers / ETL Workers: Transform raw HTML content into structured, analytics-ready records.</li> <li>Storage: MongoDB for raw/unstructured data, PostgreSQL for structured, cleaned datasets.</li> <li>Observability: Prometheus metrics, Grafana dashboards, and Loki logs for monitoring and troubleshooting.</li> <li>Deployment: Containerized services orchestrated via Kubernetes with Helm charts and CI/CD automation.</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>mkdocs.yml \u2014 Site configuration for documentation</li> <li>docs/ \u2014 Markdown pages with detailed explanations</li> <li>helm/ \u2014 Helm charts for deploying services on Kubernetes</li> <li>k8s/ \u2014 Base Kubernetes manifests for each service</li> <li>docker/ \u2014 Dockerfiles for containerizing pipeline components</li> <li>README.md \u2014 Project overview and instructions</li> </ul>"},{"location":"#why-this-project-matters","title":"Why this project matters","text":"<p>This project is portfolio-ready because it demonstrates:</p> <ul> <li>Full-stack data engineering skills \u2013 ETL, message queues, database design, monitoring, and deployment.</li> <li>Production-grade design patterns \u2013 scalability, observability, reliability, and reproducibility.</li> <li>Cloud-native approach \u2013 Kubernetes, Helm, Docker, and Terraform integration.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to the BBC News ETL Pipeline!</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch (<code>git checkout -b feature/my-feature</code>)</li> <li>Commit your changes with clear messages</li> <li>Push to your fork</li> <li>Open a Pull Request</li> </ol>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP8 guidelines</li> <li>Use black or ruff for formatting</li> <li>Write unit tests for new features or bug fixes</li> </ul>"},{"location":"faq/","title":"FAQ","text":"<p>This section answers common questions and troubleshooting scenarios for the BBC News ETL pipeline.</p>"},{"location":"faq/#general","title":"General","text":"<p>Q1. What is the purpose of this pipeline? A: To provide a production-grade, scalable ETL pipeline that scrapes BBC News articles, processes them into structured datasets, and stores them for analytics and downstream applications.</p> <p>Q2. Can I run this pipeline locally without Kubernetes? A: Yes. You can use the Docker Compose setup (<code>docker-compose.yml</code>) for local testing and development. Kubernetes is only required for production-grade deployments.</p>"},{"location":"faq/#producers-scrapers","title":"Producers (Scrapers)","text":"<p>Q3. Why do Producers need Selenium? A: Some BBC News pages use JavaScript rendering. Each Producer pod runs with its own Selenium container (e.g., <code>selenium/standalone-chrome</code>) to scrape such dynamic pages.</p> <p>Q4. How does the pipeline avoid duplicate articles? A: Before publishing messages, Producers check MongoDB for existing records. If articles already exist or counts meet a statistical threshold, they are skipped.</p> <p>Q5. How are scraping rates controlled? A: Scraping frequency is configurable via environment variables. Producers implement rate limiting, retries, and exponential backoff to prevent IP blocking.</p>"},{"location":"faq/#rabbitmq","title":"RabbitMQ","text":"<p>Q6. What happens if RabbitMQ crashes? A: All queues are configured as durable, so messages persist even if RabbitMQ restarts.</p> <p>Q7. What is the Dead Letter Queue (DLQ)? A: Messages that fail after multiple retries are redirected to the DLQ. These must be inspected and retried manually to ensure no data loss.</p>"},{"location":"faq/#consumers-etl-workers","title":"Consumers (ETL Workers)","text":"<p>Q8. What if a Consumer fails while processing a message? A: RabbitMQ will requeue the message unless it repeatedly fails, in which case it is sent to the DLQ.</p> <p>Q9. Why are both MongoDB and PostgreSQL used? A:</p> <ul> <li>MongoDB stores raw HTML and unstructured data (data lake).</li> <li>PostgreSQL stores cleaned, analytics-ready datasets (data warehouse).</li> </ul> <p>Q10. Can I scale Consumers independently? A: Yes. RabbitMQ allows multiple Consumers to process messages in parallel. With Kubernetes + KEDA, Consumers auto-scale based on queue length.</p>"},{"location":"faq/#observability","title":"Observability","text":"<p>Q11. Where can I see system health and metrics? A:</p> <ul> <li>Grafana: dashboards for queue depth, processing rate, error counts.</li> <li>Prometheus: raw metrics scraped from Producers, Consumers, and RabbitMQ.</li> <li>Loki: centralized logs (with Promtail as the agent).</li> </ul> <p>Q12. Why am I not seeing logs in Grafana Loki? A: Check that:</p> <ul> <li><code>promtail</code> agents are running.</li> <li>Log paths are correctly mounted.</li> <li>Loki service is reachable from Promtail.</li> </ul>"},{"location":"faq/#deployment","title":"Deployment","text":"<p>Q13. How are Producers scaled automatically? A: The primary Producer creates a work queue of dates. Based on queue length, KEDA scales additional Producers to handle the load.</p> <p>Q14. How can I configure environment variables for different environments (dev, staging, prod)? A: Use Helm <code>values.yaml</code> files or <code>.env</code> files in Docker Compose.</p> <p>Q15. How do I rollback a failed deployment? A:</p> <ul> <li>With Helm: <code>helm rollback &lt;release-name&gt; &lt;revision&gt;</code></li> <li>With Docker Compose: revert to the last known working image tag.</li> </ul>"},{"location":"faq/#infrastructure","title":"Infrastructure","text":"<p>Q16. Do I need cloud resources to run this? A: No. The pipeline can run locally with Docker Compose or Kubernetes (Kind/Minikube). Cloud infrastructure is optional but recommended for production.</p> <p>Q17. How is infrastructure provisioned? A: Using Terraform for declarative resource management (e.g., Kubernetes clusters, databases, networking).</p>"},{"location":"faq/#cicd","title":"CI/CD","text":"<p>Q18. How is documentation deployed automatically? A: GitHub Actions build the MkDocs site and deploy it to GitHub Pages whenever changes are pushed to <code>main</code>.</p> <p>Q19. How are Docker images versioned? A: Each Docker image is tagged with the Git commit SHA for traceability and reproducibility.</p> <p>Q20. What if my CI pipeline fails on <code>pre-commit</code> checks? A: Run <code>pre-commit run --all-files</code> locally to fix formatting and linting before pushing your changes.</p>"},{"location":"api/clients.mongo_connector/","title":"Mongo Connector API Reference","text":""},{"location":"api/clients.mongo_connector/#src.clients.mongo_connector.MongoDBOperation","title":"<code>MongoDBOperation</code>","text":"<p>Handles MongoDB operations such as connecting to a collection, inserting documents, checking for existing records, and retrieving documents grouped by date.</p> <p>Parameters:</p> Name Type Description Default <code>mongo_db_config</code> <code>MongoDBConfig</code> <p>Configuration object containing MongoDB connection details.</p> required <code>max_retries</code> <code>int</code> <p>Maximum number of retries to get a MongoDB collection.</p> <code>3</code> <code>retry_delay</code> <code>float</code> <p>Initial delay between retries in seconds. Doubles each attempt.</p> <code>1.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from src.entity.config_entity import MongoDBConfig\n&gt;&gt;&gt; from src.clients.mongo_connector import MongoDBOperation\n&gt;&gt;&gt; config = MongoDBConfig()\n&gt;&gt;&gt; with MongoDBOperation(config) as mongo_op:\n...     mongo_op.insert_data([{\"title\": \"Example\", \"published\": datetime.utcnow()}])\n...     exists = mongo_op.is_article_link_exists(\"http://example.com/article\")\n...     print(\"Article exists:\", exists)\n</code></pre> Source code in <code>src/clients/mongo_connector.py</code> <pre><code>class MongoDBOperation:\n    \"\"\"\n    Handles MongoDB operations such as connecting to a collection,\n    inserting documents, checking for existing records, and retrieving\n    documents grouped by date.\n\n    Parameters\n    ----------\n    mongo_db_config : MongoDBConfig\n        Configuration object containing MongoDB connection details.\n    max_retries : int, default=3\n        Maximum number of retries to get a MongoDB collection.\n    retry_delay : float, default=1.0\n        Initial delay between retries in seconds. Doubles each attempt.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from src.entity.config_entity import MongoDBConfig\n    &gt;&gt;&gt; from src.clients.mongo_connector import MongoDBOperation\n    &gt;&gt;&gt; config = MongoDBConfig()\n    &gt;&gt;&gt; with MongoDBOperation(config) as mongo_op:\n    ...     mongo_op.insert_data([{\"title\": \"Example\", \"published\": datetime.utcnow()}])\n    ...     exists = mongo_op.is_article_link_exists(\"http://example.com/article\")\n    ...     print(\"Article exists:\", exists)\n    \"\"\"\n\n    def __init__(self, mongo_db_config: MongoDBConfig, max_retries: int = 3, retry_delay: float = 1.0):\n        \"\"\"\n        Initialize MongoDBOperation with config and establish MongoDB client connection.\n\n        Parameters\n        ----------\n        mongo_db_config : MongoDBConfig\n            Configuration object with MongoDB settings.\n        max_retries : int, default=3\n            Maximum retry attempts for collection retrieval.\n        retry_delay : float, default=1.0\n            Initial delay between retries (seconds).\n\n        Raises\n        ------\n        CustomException\n            If connection fails.\n        \"\"\"\n        self.mongo_db_config = mongo_db_config\n        self.max_retries = max_retries\n        self.retry_delay = retry_delay\n        self.client: Optional[MongoClient] = None\n        self._collection = None\n\n        self._connect()\n\n\n    def _connect(self):\n        \"\"\"\n        Establish a MongoDB client connection.\n\n        Raises\n        ------\n        CustomException\n            If connection to MongoDB fails.\n        \"\"\"\n        try:\n            logging.info(\"Establishing MongoDB client connection...\")\n            self.client = MongoClient(\n                self.mongo_db_config.MONGODB_URI,\n                serverSelectionTimeoutMS=5000  # Fast fail if not reachable\n            )\n            # Test the connection\n            self.client.admin.command(\"ping\")\n            logging.info(\"MongoDB connection established successfully.\")\n        except ConnectionFailure as e:\n            logging.error(f\"MongoDB connection failed: {e}\")\n            raise CustomException(\"Failed to connect to MongoDB\", sys)\n\n\n    def _get_collection(self) -&gt; Collection:\n        \"\"\"\n        Retrieve the MongoDB collection with retry logic.\n\n        Returns\n        -------\n        Collection\n            The MongoDB collection object.\n\n        Raises\n        ------\n        CustomException\n            If retrieval fails after retries.\n        \"\"\"\n        if self._collection:\n            return self._collection\n\n        for attempt in range(1, self.max_retries + 1):\n            try:\n                logging.info(f\"[Attempt {attempt}] Getting collection: \"\n                             f\"{self.mongo_db_config.MONGODB_COLLECTION_NAME}\")\n\n                if self.client is None:\n                    raise CustomException(\"MongoDB client is not initialized\", sys)\n\n                db = self.client[self.mongo_db_config.MONGODB_DATABASE_NAME]\n                collection = db[self.mongo_db_config.MONGODB_COLLECTION_NAME]\n\n                # Ping the collection to validate access\n                collection.estimated_document_count()\n                logging.info(\"MongoDB collection retrieved successfully.\")\n                self._collection = collection\n                return collection\n            except PyMongoError as e:\n                logging.warning(f\"Attempt {attempt} failed: {e}\")\n                if attempt &lt; self.max_retries:\n                    time.sleep(self.retry_delay * (2 ** (attempt - 1)))\n                else:\n                    logging.error(\"Exceeded maximum retries for getting collection.\")\n                    raise CustomException(\"Failed to get MongoDB collection\", sys)\n\n\n    @property\n    def collection(self) -&gt; Collection:\n        \"\"\"\n        MongoDB collection instance (lazy-loaded with retry).\n\n        Returns\n        -------\n        Collection\n            MongoDB collection object.\n        \"\"\"\n        return self._get_collection()\n\n\n    def get_date_wise_doc_count(self) -&gt; Optional[List[Tuple[datetime, int]]]:\n        \"\"\"\n        Get the dates and respective document counts, ordered by date.\n\n        Returns\n        -------\n        list of tuple of (datetime.date, int) or None\n            List of tuples containing (date, document count).\n            Returns None if no documents are found.\n\n        Raises\n        ------\n        CustomException\n            If query execution fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; counts = mongo_op.get_date_wise_doc_count()\n        &gt;&gt;&gt; if counts:\n        ...     for date, count in counts:\n        ...         print(date, count)\n        \"\"\"\n        try:\n            pipeline = [\n                            {\n                                \"$group\": {\n                                    \"_id\": {  # Extract just the date part (Y-M-D)\n                                        \"$dateToString\": {\n                                            \"format\": \"%Y-%m-%d\",\n                                            \"date\": \"$published\"\n                                        }\n                                    },\n                                    \"documents\": { \"$push\": \"$$ROOT\" },  # Group all documents for the same date\n                                    \"count\": { \"$sum\": 1 }  # Optional: count of documents per date\n                                }\n                            },\n                            {\n                                \"$sort\": { \"_id\": 1 }  # Sort by date ascending\n                            }\n                        ]\n            result = list(self.collection.aggregate(pipeline))\n            date_wise_count = None\n            if result:\n                # Convert _id from string to datetime.date\n                date_wise_count = [\n                    (group['_id'], group['count'])\n                    for group in result\n                ]\n                logging.debug(f\"Fetched latest document: {date_wise_count}\")\n                return date_wise_count\n            else:\n                logging.info(f\"No documents found : {date_wise_count}.\")\n                return date_wise_count\n        except Exception as e:\n            logging.error(f\"Error fetching latest date: {e}\")\n            raise CustomException(e, sys)\n\n\n    def insert_data(self, data: List[Dict]):\n        \"\"\"\n        Insert multiple documents into the MongoDB collection.\n\n        Parameters\n        ----------\n        data : list of dict\n            Documents to insert.\n\n        Raises\n        ------\n        CustomException\n            If insertion fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; documents = [\n        ...     {\"title\": \"Breaking News\", \"published\": datetime(2023, 8, 25)},\n        ...     {\"title\": \"Tech Update\", \"published\": datetime(2023, 8, 26)}\n        ... ]\n        &gt;&gt;&gt; mongo_op.insert_data(documents)\n        \"\"\"\n        try:\n            logging.info(f\"Inserting data to MongoDB collection {self.mongo_db_config.MONGODB_COLLECTION_NAME}\")\n            self.collection.insert_many(data)\n        except Exception as e:\n            logging.error(f\"Error while inserting data to MongoDB: {e}\")\n            raise CustomException(e, sys)\n\n\n    def is_article_link_exists(self, article_link: str):\n        \"\"\"\n        Check if a document with the given article link exists.\n\n        Parameters\n        ----------\n        article_link : str\n            URL of the article.\n\n        Returns\n        -------\n        bool\n            True if the document exists, False otherwise.\n\n        Raises\n        ------\n        CustomException\n            If query fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; exists = mongo_op.is_article_link_exists(\"http://example.com/article123\")\n        &gt;&gt;&gt; print(exists)\n        True\n        \"\"\"\n        try:\n            doc_url = self.collection.find_one(\n                filter={\n                    self.mongo_db_config.MONGO_DOC_ARTICLE_URL_KEY: article_link,\n                }\n            )\n\n            if doc_url is not None:\n                logging.info(f\"Document exists for Url: {article_link}\")\n                return True\n            else:\n                logging.info(f\"No document exists for Url: {article_link}\")\n                return False\n        except Exception as e:\n            logging.error(f\"Error while fetching data from MongoDB: {e}\")\n            raise CustomException(e, sys)\n\n\n    def close_connection(self):\n        \"\"\"\n        Gracefully close the MongoDB client connection.\n\n        Raises\n        ------\n        CustomException\n            If closing the connection fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; mongo_op.close_connection()\n        \"\"\"\n        try:\n            if self.client:\n                logging.info(\"Closing MongoDB client connection...\")\n                self.client.close()\n                self.client = None\n                self._collection = None\n                logging.info(\"MongoDB connection closed.\")\n        except Exception as e:\n            logging.error(f\"Error closing MongoDB connection: {e}\")\n            raise CustomException(e, sys)\n\n    def __enter__(self):\n        \"\"\"\n        Enter runtime context (for `with` statement).\n\n        Returns\n        -------\n        MongoDBOperation\n            The MongoDBOperation instance.\n\n        Examples\n        --------\n        &gt;&gt;&gt; with MongoDBOperation(config) as mongo_op:\n        ...     mongo_op.insert_data([{\"title\": \"inside context\"}])\n        \"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"\n        Exit runtime context and close MongoDB connection.\n\n        Parameters\n        ----------\n        exc_type : type\n            Exception type (if raised inside context).\n        exc_val : Exception\n            Exception instance (if raised).\n        exc_tb : traceback\n            Traceback object.\n        \"\"\"\n        self.close_connection()\n</code></pre>"},{"location":"api/clients.mongo_connector/#src.clients.mongo_connector.MongoDBOperation.collection","title":"<code>collection</code>","text":"<p>MongoDB collection instance (lazy-loaded with retry).</p> <p>Returns:</p> Type Description <code>Collection</code> <p>MongoDB collection object.</p>"},{"location":"api/clients.mongo_connector/#src.clients.mongo_connector.MongoDBOperation.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter runtime context (for <code>with</code> statement).</p> <p>Returns:</p> Type Description <code>MongoDBOperation</code> <p>The MongoDBOperation instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with MongoDBOperation(config) as mongo_op:\n...     mongo_op.insert_data([{\"title\": \"inside context\"}])\n</code></pre> Source code in <code>src/clients/mongo_connector.py</code> <pre><code>def __enter__(self):\n    \"\"\"\n    Enter runtime context (for `with` statement).\n\n    Returns\n    -------\n    MongoDBOperation\n        The MongoDBOperation instance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; with MongoDBOperation(config) as mongo_op:\n    ...     mongo_op.insert_data([{\"title\": \"inside context\"}])\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/clients.mongo_connector/#src.clients.mongo_connector.MongoDBOperation.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Exit runtime context and close MongoDB connection.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <code>type</code> <p>Exception type (if raised inside context).</p> required <code>exc_val</code> <code>Exception</code> <p>Exception instance (if raised).</p> required <code>exc_tb</code> <code>traceback</code> <p>Traceback object.</p> required Source code in <code>src/clients/mongo_connector.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"\n    Exit runtime context and close MongoDB connection.\n\n    Parameters\n    ----------\n    exc_type : type\n        Exception type (if raised inside context).\n    exc_val : Exception\n        Exception instance (if raised).\n    exc_tb : traceback\n        Traceback object.\n    \"\"\"\n    self.close_connection()\n</code></pre>"},{"location":"api/clients.mongo_connector/#src.clients.mongo_connector.MongoDBOperation.__init__","title":"<code>__init__(mongo_db_config, max_retries=3, retry_delay=1.0)</code>","text":"<p>Initialize MongoDBOperation with config and establish MongoDB client connection.</p> <p>Parameters:</p> Name Type Description Default <code>mongo_db_config</code> <code>MongoDBConfig</code> <p>Configuration object with MongoDB settings.</p> required <code>max_retries</code> <code>int</code> <p>Maximum retry attempts for collection retrieval.</p> <code>3</code> <code>retry_delay</code> <code>float</code> <p>Initial delay between retries (seconds).</p> <code>1.0</code> <p>Raises:</p> Type Description <code>CustomException</code> <p>If connection fails.</p> Source code in <code>src/clients/mongo_connector.py</code> <pre><code>def __init__(self, mongo_db_config: MongoDBConfig, max_retries: int = 3, retry_delay: float = 1.0):\n    \"\"\"\n    Initialize MongoDBOperation with config and establish MongoDB client connection.\n\n    Parameters\n    ----------\n    mongo_db_config : MongoDBConfig\n        Configuration object with MongoDB settings.\n    max_retries : int, default=3\n        Maximum retry attempts for collection retrieval.\n    retry_delay : float, default=1.0\n        Initial delay between retries (seconds).\n\n    Raises\n    ------\n    CustomException\n        If connection fails.\n    \"\"\"\n    self.mongo_db_config = mongo_db_config\n    self.max_retries = max_retries\n    self.retry_delay = retry_delay\n    self.client: Optional[MongoClient] = None\n    self._collection = None\n\n    self._connect()\n</code></pre>"},{"location":"api/clients.mongo_connector/#src.clients.mongo_connector.MongoDBOperation.close_connection","title":"<code>close_connection()</code>","text":"<p>Gracefully close the MongoDB client connection.</p> <p>Raises:</p> Type Description <code>CustomException</code> <p>If closing the connection fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mongo_op.close_connection()\n</code></pre> Source code in <code>src/clients/mongo_connector.py</code> <pre><code>def close_connection(self):\n    \"\"\"\n    Gracefully close the MongoDB client connection.\n\n    Raises\n    ------\n    CustomException\n        If closing the connection fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mongo_op.close_connection()\n    \"\"\"\n    try:\n        if self.client:\n            logging.info(\"Closing MongoDB client connection...\")\n            self.client.close()\n            self.client = None\n            self._collection = None\n            logging.info(\"MongoDB connection closed.\")\n    except Exception as e:\n        logging.error(f\"Error closing MongoDB connection: {e}\")\n        raise CustomException(e, sys)\n</code></pre>"},{"location":"api/clients.mongo_connector/#src.clients.mongo_connector.MongoDBOperation.get_date_wise_doc_count","title":"<code>get_date_wise_doc_count()</code>","text":"<p>Get the dates and respective document counts, ordered by date.</p> <p>Returns:</p> Type Description <code>list of tuple of (datetime.date, int) or None</code> <p>List of tuples containing (date, document count). Returns None if no documents are found.</p> <p>Raises:</p> Type Description <code>CustomException</code> <p>If query execution fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; counts = mongo_op.get_date_wise_doc_count()\n&gt;&gt;&gt; if counts:\n...     for date, count in counts:\n...         print(date, count)\n</code></pre> Source code in <code>src/clients/mongo_connector.py</code> <pre><code>def get_date_wise_doc_count(self) -&gt; Optional[List[Tuple[datetime, int]]]:\n    \"\"\"\n    Get the dates and respective document counts, ordered by date.\n\n    Returns\n    -------\n    list of tuple of (datetime.date, int) or None\n        List of tuples containing (date, document count).\n        Returns None if no documents are found.\n\n    Raises\n    ------\n    CustomException\n        If query execution fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; counts = mongo_op.get_date_wise_doc_count()\n    &gt;&gt;&gt; if counts:\n    ...     for date, count in counts:\n    ...         print(date, count)\n    \"\"\"\n    try:\n        pipeline = [\n                        {\n                            \"$group\": {\n                                \"_id\": {  # Extract just the date part (Y-M-D)\n                                    \"$dateToString\": {\n                                        \"format\": \"%Y-%m-%d\",\n                                        \"date\": \"$published\"\n                                    }\n                                },\n                                \"documents\": { \"$push\": \"$$ROOT\" },  # Group all documents for the same date\n                                \"count\": { \"$sum\": 1 }  # Optional: count of documents per date\n                            }\n                        },\n                        {\n                            \"$sort\": { \"_id\": 1 }  # Sort by date ascending\n                        }\n                    ]\n        result = list(self.collection.aggregate(pipeline))\n        date_wise_count = None\n        if result:\n            # Convert _id from string to datetime.date\n            date_wise_count = [\n                (group['_id'], group['count'])\n                for group in result\n            ]\n            logging.debug(f\"Fetched latest document: {date_wise_count}\")\n            return date_wise_count\n        else:\n            logging.info(f\"No documents found : {date_wise_count}.\")\n            return date_wise_count\n    except Exception as e:\n        logging.error(f\"Error fetching latest date: {e}\")\n        raise CustomException(e, sys)\n</code></pre>"},{"location":"api/clients.mongo_connector/#src.clients.mongo_connector.MongoDBOperation.insert_data","title":"<code>insert_data(data)</code>","text":"<p>Insert multiple documents into the MongoDB collection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list of dict</code> <p>Documents to insert.</p> required <p>Raises:</p> Type Description <code>CustomException</code> <p>If insertion fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; documents = [\n...     {\"title\": \"Breaking News\", \"published\": datetime(2023, 8, 25)},\n...     {\"title\": \"Tech Update\", \"published\": datetime(2023, 8, 26)}\n... ]\n&gt;&gt;&gt; mongo_op.insert_data(documents)\n</code></pre> Source code in <code>src/clients/mongo_connector.py</code> <pre><code>def insert_data(self, data: List[Dict]):\n    \"\"\"\n    Insert multiple documents into the MongoDB collection.\n\n    Parameters\n    ----------\n    data : list of dict\n        Documents to insert.\n\n    Raises\n    ------\n    CustomException\n        If insertion fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; documents = [\n    ...     {\"title\": \"Breaking News\", \"published\": datetime(2023, 8, 25)},\n    ...     {\"title\": \"Tech Update\", \"published\": datetime(2023, 8, 26)}\n    ... ]\n    &gt;&gt;&gt; mongo_op.insert_data(documents)\n    \"\"\"\n    try:\n        logging.info(f\"Inserting data to MongoDB collection {self.mongo_db_config.MONGODB_COLLECTION_NAME}\")\n        self.collection.insert_many(data)\n    except Exception as e:\n        logging.error(f\"Error while inserting data to MongoDB: {e}\")\n        raise CustomException(e, sys)\n</code></pre>"},{"location":"api/clients.mongo_connector/#src.clients.mongo_connector.MongoDBOperation.is_article_link_exists","title":"<code>is_article_link_exists(article_link)</code>","text":"<p>Check if a document with the given article link exists.</p> <p>Parameters:</p> Name Type Description Default <code>article_link</code> <code>str</code> <p>URL of the article.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the document exists, False otherwise.</p> <p>Raises:</p> Type Description <code>CustomException</code> <p>If query fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; exists = mongo_op.is_article_link_exists(\"http://example.com/article123\")\n&gt;&gt;&gt; print(exists)\nTrue\n</code></pre> Source code in <code>src/clients/mongo_connector.py</code> <pre><code>def is_article_link_exists(self, article_link: str):\n    \"\"\"\n    Check if a document with the given article link exists.\n\n    Parameters\n    ----------\n    article_link : str\n        URL of the article.\n\n    Returns\n    -------\n    bool\n        True if the document exists, False otherwise.\n\n    Raises\n    ------\n    CustomException\n        If query fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; exists = mongo_op.is_article_link_exists(\"http://example.com/article123\")\n    &gt;&gt;&gt; print(exists)\n    True\n    \"\"\"\n    try:\n        doc_url = self.collection.find_one(\n            filter={\n                self.mongo_db_config.MONGO_DOC_ARTICLE_URL_KEY: article_link,\n            }\n        )\n\n        if doc_url is not None:\n            logging.info(f\"Document exists for Url: {article_link}\")\n            return True\n        else:\n            logging.info(f\"No document exists for Url: {article_link}\")\n            return False\n    except Exception as e:\n        logging.error(f\"Error while fetching data from MongoDB: {e}\")\n        raise CustomException(e, sys)\n</code></pre>"},{"location":"api/clients.postgress_connector/","title":"Postgress Connector API Reference","text":""},{"location":"api/clients.postgress_connector/#src.clients.postgress_connector.PostgreSQLOperation","title":"<code>PostgreSQLOperation</code>","text":"<p>PostgreSQLOperation handles PostgreSQL database operations safely with retries, context-managed cursors, table creation, and CRUD operations with proper logging.</p> <p>Attributes:</p> Name Type Description <code>postgres_config</code> <code>PostgresDBConfig</code> <p>Configuration object containing PostgreSQL connection details.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from src.entity.config_entity import PostgresDBConfig\n&gt;&gt;&gt; config = PostgresDBConfig()\n&gt;&gt;&gt; db_client = PostgreSQLOperation(config)\n&gt;&gt;&gt; db_client.create_table()\n&gt;&gt;&gt; article = {\n...     \"title\": \"Sample Article\",\n...     \"author\": \"John Doe\",\n...     \"source\": \"BBC\",\n...     \"published_date\": \"2025-09-27T10:00:00Z\",\n...     \"scraped_date\": \"2025-09-27T11:00:00Z\",\n...     \"summary\": \"This is a summary\",\n...     \"content\": \"Full content here\",\n...     \"url\": \"https://example.com/article1\",\n...     \"category\": \"News\"\n... }\n&gt;&gt;&gt; db_client.insert_article(article)\n&gt;&gt;&gt; db_client.table_exists()\nTrue\n</code></pre> Source code in <code>src/clients/postgress_connector.py</code> <pre><code>class PostgreSQLOperation:\n    \"\"\"\n    PostgreSQLOperation handles PostgreSQL database operations safely with retries, context-managed cursors,\n    table creation, and CRUD operations with proper logging.\n\n    Attributes\n    ----------\n    postgres_config : PostgresDBConfig\n        Configuration object containing PostgreSQL connection details.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from src.entity.config_entity import PostgresDBConfig\n    &gt;&gt;&gt; config = PostgresDBConfig()\n    &gt;&gt;&gt; db_client = PostgreSQLOperation(config)\n    &gt;&gt;&gt; db_client.create_table()\n    &gt;&gt;&gt; article = {\n    ...     \"title\": \"Sample Article\",\n    ...     \"author\": \"John Doe\",\n    ...     \"source\": \"BBC\",\n    ...     \"published_date\": \"2025-09-27T10:00:00Z\",\n    ...     \"scraped_date\": \"2025-09-27T11:00:00Z\",\n    ...     \"summary\": \"This is a summary\",\n    ...     \"content\": \"Full content here\",\n    ...     \"url\": \"https://example.com/article1\",\n    ...     \"category\": \"News\"\n    ... }\n    &gt;&gt;&gt; db_client.insert_article(article)\n    &gt;&gt;&gt; db_client.table_exists()\n    True\n    \"\"\"\n    def __init__(self, postgres_config: PostgresDBConfig):\n        \"\"\"\n        Initialize PostgreSQLOperation with configuration.\n\n        Parameters\n        ----------\n        postgres_config : PostgresDBConfig\n            Contains all necessary DB connection info like host, port, user, password, and table name.\n        \"\"\"\n        self.postgres_config = postgres_config\n\n    @retry(wait=wait_fixed(2), stop=stop_after_attempt(5))\n    def get_connection(self):\n        \"\"\"\n        Establish a connection to PostgreSQL with retry logic.\n\n        Returns\n        -------\n        connection : psycopg2.extensions.connection\n            Active connection object.\n\n        Raises\n        ------\n        psycopg2.OperationalError\n            If connection cannot be established after retries.\n\n        Examples\n        --------\n        &gt;&gt;&gt; conn = db_client.get_connection()\n        &gt;&gt;&gt; conn.closed\n        0\n        \"\"\"\n        return psycopg2.connect(\n            dbname=self.postgres_config.POSTGRES_DATABASE_NAME,\n            user=self.postgres_config.POSTGRES_USER,\n            password=self.postgres_config.POSTGRES_PASSWORD,\n            host=self.postgres_config.POSTGRES_HOST,\n            port=self.postgres_config.POSTGRES_PORT\n        )\n\n    @contextmanager\n    def connection_cursor(self):\n        \"\"\"\n        Context manager for PostgreSQL connection and cursor.\n\n        Ensures commit on success and rollback on failure.\n\n        Yields\n        ------\n        conn : psycopg2.extensions.connection\n        cursor : psycopg2.extensions.cursor\n\n        Raises\n        ------\n        Exception\n            Any exception during DB operation is propagated after rollback.\n\n        Examples\n        --------\n        &gt;&gt;&gt; with db_client.connection_cursor() as (conn, cursor):\n        ...     cursor.execute(\"SELECT 1;\")\n        ...     cursor.fetchone()\n        (1,)\n        \"\"\"\n        start_time = time.perf_counter()\n        conn = self.get_connection()\n        cursor = conn.cursor()\n        try:\n            yield conn, cursor\n            conn.commit()\n        except Exception as e:\n            conn.rollback()\n            logger.error(\n                \"Database operation failed\",\n                extra={\n                    \"service\": service_name,\n                    \"host\": self.postgres_config.POSTGRES_HOST,\n                    \"stack_trace\": str(e),\n                    \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n            raise\n        finally:\n            cursor.close()\n            conn.close()\n\n    def table_exists(self) -&gt; bool:\n        \"\"\"\n        Check if the target table exists in the 'public' schema.\n\n        Returns\n        -------\n        exists : bool\n            True if table exists, False otherwise.\n\n        Raises\n        ------\n        CustomException\n            If the check fails due to a DB error.\n\n        Examples\n        --------\n        &gt;&gt;&gt; db_client.table_exists()\n        True\n        \"\"\"\n        check_query = \"\"\"\n        SELECT EXISTS (\n            SELECT FROM information_schema.tables\n            WHERE table_schema = 'public' AND table_name = %s\n        );\n        \"\"\"\n        start_time = time.perf_counter()\n        with self.connection_cursor() as (_, cursor):\n            try:\n                cursor.execute(check_query, (self.postgres_config.POSTGRES_TABLE_NAME,))\n                exists = cursor.fetchone()[0]\n                logger.info(\n                    f\"Table '{self.postgres_config.POSTGRES_TABLE_NAME}' exists: {exists}\",\n                    extra={\n                        \"service\": service_name,\n                        \"host\": self.postgres_config.POSTGRES_HOST,\n                        \"duration_ms\": calculate_duration(start_time),\n                    }\n                )\n                return exists\n            except Exception as e:\n                logger.error(\n                    \"Failed to check if table exists\",\n                    extra={\n                        \"service\": service_name,\n                        \"host\": self.postgres_config.POSTGRES_HOST,\n                        \"stack_trace\": str(e),\n                        \"duration_ms\": calculate_duration(start_time),\n                    }\n                )\n                raise CustomException(\"Table existence check failed\", e)\n\n    def create_table(self):\n        \"\"\"\n        Create the main articles table if it does not exist.\n\n        Table structure:\n        - id : SERIAL PRIMARY KEY\n        - title : TEXT NOT NULL\n        - author : TEXT\n        - source : TEXT\n        - publish_date : TIMESTAMP\n        - scraped_date : TIMESTAMP\n        - summary : TEXT\n        - content : TEXT\n        - url : TEXT UNIQUE NOT NULL\n        - category : TEXT\n        - created_at : TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n\n        Raises\n        ------\n        psycopg2.Error\n            If table creation fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; db_client.create_table()\n        Table 'articles' ensured to exist.\n        \"\"\"\n\n        start_time = time.perf_counter()\n        create_table_query = sql.SQL(\"\"\"\n        CREATE TABLE IF NOT EXISTS {table} (\n            id SERIAL PRIMARY KEY,\n            title TEXT NOT NULL,\n            author TEXT,\n            source TEXT,\n            publish_date TIMESTAMP,\n            scraped_date TIMESTAMP,\n            summary TEXT,\n            content TEXT,\n            url TEXT UNIQUE NOT NULL,\n            category TEXT,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        );\n        \"\"\").format(\n                table=sql.Identifier(self.postgres_config.POSTGRES_TABLE_NAME)\n            )\n        with self.connection_cursor() as (_, cursor):\n            cursor.execute(create_table_query)\n            logger.info(\n                f\"Table {self.postgres_config.POSTGRES_TABLE_NAME} ensured to exist.\",\n                extra={\n                        \"service\": service_name,\n                        \"host\": self.postgres_config.POSTGRES_HOST,\n                        \"duration_ms\": calculate_duration(start_time),\n                    }\n                )\n\n    def insert_article(self, article_data: Dict):\n        \"\"\"\n        Insert an article into the database.\n\n        Uses `ON CONFLICT (url) DO NOTHING` to avoid duplicates.\n\n        Parameters\n        ----------\n        article_data : dict\n            Dictionary containing keys:\n            'title', 'author', 'source', 'published_date', 'scraped_date',\n            'summary', 'content', 'url', 'category'\n\n        Raises\n        ------\n        CustomException\n            If insertion fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; article = {\n        ...     \"title\": \"Test\",\n        ...     \"author\": \"Alice\",\n        ...     \"source\": \"BBC\",\n        ...     \"published_date\": \"2025-09-27T10:00:00Z\",\n        ...     \"scraped_date\": \"2025-09-27T11:00:00Z\",\n        ...     \"summary\": \"Test summary\",\n        ...     \"content\": \"Full content\",\n        ...     \"url\": \"https://example.com/test\",\n        ...     \"category\": \"News\"\n        ... }\n        &gt;&gt;&gt; db_client.insert_article(article)\n        Article inserted: https://example.com/test\n        \"\"\"\n\n        start_time = time.perf_counter()\n        insert_query = sql.SQL(\"\"\"\n        INSERT INTO {table}\n        (title, author, source, publish_date, scraped_date, summary, content, url, category)\n        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n        ON CONFLICT (url) DO NOTHING\n        \"\"\").format(\n                table=sql.Identifier(self.postgres_config.POSTGRES_TABLE_NAME)\n            )\n        try:\n            values = (\n                article_data.get(\"title\"),\n                article_data.get(\"author\"),\n                article_data.get(\"source\"),\n                self._parse_date(article_data.get(\"published_date\")),\n                self._parse_date(article_data.get(\"scraped_date\")),\n                article_data.get(\"summary\"),\n                article_data.get(\"content\"),\n                article_data.get(\"url\"),\n                article_data.get(\"category\"),\n            )\n            with self.connection_cursor() as (_, cursor):\n                cursor.execute(insert_query, values)\n                logger.info(\n                    f\"Article inserted: {article_data.get('url')}\",\n                    extra={\n                        \"service\": service_name,\n                        \"host\": self.postgres_config.POSTGRES_HOST,\n                        \"duration_ms\": calculate_duration(start_time),\n                    }\n                )\n        except Exception as e:\n            logger.error(\n                \"Failed to insert article\",\n                extra={\n                    \"service\": service_name,\n                    \"host\": self.postgres_config.POSTGRES_HOST,\n                    \"stack_trace\": str(e),\n                    \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n            raise CustomException(\"Article insert failed\", e)\n\n    def _parse_date(self, date_str):\n        \"\"\"\n        Parse ISO 8601 datetime string to Python datetime object.\n\n        Parameters\n        ----------\n        date_str : str\n            Datetime string, e.g. '2025-09-27T10:00:00Z'\n\n        Returns\n        -------\n        datetime or None\n            Parsed datetime object or None if invalid or empty.\n\n        Examples\n        --------\n        &gt;&gt;&gt; db_client._parse_date(\"2025-09-27T10:00:00Z\")\n        datetime.datetime(2025, 9, 27, 10, 0, tzinfo=datetime.timezone.utc)\n        &gt;&gt;&gt; db_client._parse_date(None)\n        None\n        \"\"\"\n\n        start_time = time.perf_counter()\n        if not date_str:\n            return None\n        try:\n            return datetime.fromisoformat(date_str.replace(\"Z\", \"+00:00\"))\n        except ValueError as e:\n            logger.warning(\n                \"Invalid datetime format\",\n                extra={\n                    \"service\": service_name,\n                    \"host\": self.postgres_config.POSTGRES_HOST,\n                    \"stack_trace\": str(e),\n                    \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n            return None\n\n    def create_test_table(self):\n        start_time = time.perf_counter()\n        create_table_query = (\"\"\"\n        CREATE TABLE IF NOT EXISTS {table} (\n            id SERIAL PRIMARY KEY,\n            url TEXT UNIQUE NOT NULL,\n        );\n        \"\"\").format(\n            table = sql.Identifier(self.postgres_config.POSTGRES_TABLE_NAME)\n        )\n        with self.connection_cursor() as (_, cursor):\n            cursor.execute(create_table_query)\n            logger.info(\n                f\"Table {self.postgres_config.POSTGRES_TABLE_NAME} ensured to exist.\",\n                extra={\n                        \"service\": service_name,\n                        \"host\": self.postgres_config.POSTGRES_HOST,\n                        \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n\n\n    def insert_test_article(self, article_data: Dict):\n        start_time = time.perf_counter()\n        insert_query = sql.SQL(\"\"\"\n        INSERT INTO {table}\n        (id, url)\n        VALUES (%s, %s)\n        ON CONFLICT (url) DO NOTHING\n        \"\"\").format(\n            table=sql.Identifier(self.postgres_config.POSTGRES_TABLE_NAME)\n            )\n        try:\n            values = (\n                article_data.get(\"task_id\"),\n                article_data.get(\"url\"),\n            )\n            with self.connection_cursor() as (_, cursor):\n                cursor.execute(insert_query, values)\n                logger.info(\n                    f\"Article inserted: {article_data.get('url')}\",\n                    extra={\n                        \"service\": service_name,\n                        \"host\": self.postgres_config.POSTGRES_HOST,\n                        \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n        except Exception as e:\n            logger.error(\n                \"Failed to insert article\",\n                extra={\n                    \"service\": service_name,\n                    \"host\": self.postgres_config.POSTGRES_HOST,\n                    \"stack_trace\": str(e),\n                    \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n            raise CustomException(\"Article insert failed\", e)\n</code></pre>"},{"location":"api/clients.postgress_connector/#src.clients.postgress_connector.PostgreSQLOperation.__init__","title":"<code>__init__(postgres_config)</code>","text":"<p>Initialize PostgreSQLOperation with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>postgres_config</code> <code>PostgresDBConfig</code> <p>Contains all necessary DB connection info like host, port, user, password, and table name.</p> required Source code in <code>src/clients/postgress_connector.py</code> <pre><code>def __init__(self, postgres_config: PostgresDBConfig):\n    \"\"\"\n    Initialize PostgreSQLOperation with configuration.\n\n    Parameters\n    ----------\n    postgres_config : PostgresDBConfig\n        Contains all necessary DB connection info like host, port, user, password, and table name.\n    \"\"\"\n    self.postgres_config = postgres_config\n</code></pre>"},{"location":"api/clients.postgress_connector/#src.clients.postgress_connector.PostgreSQLOperation.connection_cursor","title":"<code>connection_cursor()</code>","text":"<p>Context manager for PostgreSQL connection and cursor.</p> <p>Ensures commit on success and rollback on failure.</p> <p>Yields:</p> Name Type Description <code>conn</code> <code>connection</code> <code>cursor</code> <code>cursor</code> <p>Raises:</p> Type Description <code>Exception</code> <p>Any exception during DB operation is propagated after rollback.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with db_client.connection_cursor() as (conn, cursor):\n...     cursor.execute(\"SELECT 1;\")\n...     cursor.fetchone()\n(1,)\n</code></pre> Source code in <code>src/clients/postgress_connector.py</code> <pre><code>@contextmanager\ndef connection_cursor(self):\n    \"\"\"\n    Context manager for PostgreSQL connection and cursor.\n\n    Ensures commit on success and rollback on failure.\n\n    Yields\n    ------\n    conn : psycopg2.extensions.connection\n    cursor : psycopg2.extensions.cursor\n\n    Raises\n    ------\n    Exception\n        Any exception during DB operation is propagated after rollback.\n\n    Examples\n    --------\n    &gt;&gt;&gt; with db_client.connection_cursor() as (conn, cursor):\n    ...     cursor.execute(\"SELECT 1;\")\n    ...     cursor.fetchone()\n    (1,)\n    \"\"\"\n    start_time = time.perf_counter()\n    conn = self.get_connection()\n    cursor = conn.cursor()\n    try:\n        yield conn, cursor\n        conn.commit()\n    except Exception as e:\n        conn.rollback()\n        logger.error(\n            \"Database operation failed\",\n            extra={\n                \"service\": service_name,\n                \"host\": self.postgres_config.POSTGRES_HOST,\n                \"stack_trace\": str(e),\n                \"duration_ms\": calculate_duration(start_time),\n            }\n        )\n        raise\n    finally:\n        cursor.close()\n        conn.close()\n</code></pre>"},{"location":"api/clients.postgress_connector/#src.clients.postgress_connector.PostgreSQLOperation.create_table","title":"<code>create_table()</code>","text":"<p>Create the main articles table if it does not exist.</p> <p>Table structure: - id : SERIAL PRIMARY KEY - title : TEXT NOT NULL - author : TEXT - source : TEXT - publish_date : TIMESTAMP - scraped_date : TIMESTAMP - summary : TEXT - content : TEXT - url : TEXT UNIQUE NOT NULL - category : TEXT - created_at : TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p> <p>Raises:</p> Type Description <code>Error</code> <p>If table creation fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; db_client.create_table()\nTable 'articles' ensured to exist.\n</code></pre> Source code in <code>src/clients/postgress_connector.py</code> <pre><code>def create_table(self):\n    \"\"\"\n    Create the main articles table if it does not exist.\n\n    Table structure:\n    - id : SERIAL PRIMARY KEY\n    - title : TEXT NOT NULL\n    - author : TEXT\n    - source : TEXT\n    - publish_date : TIMESTAMP\n    - scraped_date : TIMESTAMP\n    - summary : TEXT\n    - content : TEXT\n    - url : TEXT UNIQUE NOT NULL\n    - category : TEXT\n    - created_at : TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n\n    Raises\n    ------\n    psycopg2.Error\n        If table creation fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; db_client.create_table()\n    Table 'articles' ensured to exist.\n    \"\"\"\n\n    start_time = time.perf_counter()\n    create_table_query = sql.SQL(\"\"\"\n    CREATE TABLE IF NOT EXISTS {table} (\n        id SERIAL PRIMARY KEY,\n        title TEXT NOT NULL,\n        author TEXT,\n        source TEXT,\n        publish_date TIMESTAMP,\n        scraped_date TIMESTAMP,\n        summary TEXT,\n        content TEXT,\n        url TEXT UNIQUE NOT NULL,\n        category TEXT,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    );\n    \"\"\").format(\n            table=sql.Identifier(self.postgres_config.POSTGRES_TABLE_NAME)\n        )\n    with self.connection_cursor() as (_, cursor):\n        cursor.execute(create_table_query)\n        logger.info(\n            f\"Table {self.postgres_config.POSTGRES_TABLE_NAME} ensured to exist.\",\n            extra={\n                    \"service\": service_name,\n                    \"host\": self.postgres_config.POSTGRES_HOST,\n                    \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n</code></pre>"},{"location":"api/clients.postgress_connector/#src.clients.postgress_connector.PostgreSQLOperation.get_connection","title":"<code>get_connection()</code>","text":"<p>Establish a connection to PostgreSQL with retry logic.</p> <p>Returns:</p> Name Type Description <code>connection</code> <code>connection</code> <p>Active connection object.</p> <p>Raises:</p> Type Description <code>OperationalError</code> <p>If connection cannot be established after retries.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; conn = db_client.get_connection()\n&gt;&gt;&gt; conn.closed\n0\n</code></pre> Source code in <code>src/clients/postgress_connector.py</code> <pre><code>@retry(wait=wait_fixed(2), stop=stop_after_attempt(5))\ndef get_connection(self):\n    \"\"\"\n    Establish a connection to PostgreSQL with retry logic.\n\n    Returns\n    -------\n    connection : psycopg2.extensions.connection\n        Active connection object.\n\n    Raises\n    ------\n    psycopg2.OperationalError\n        If connection cannot be established after retries.\n\n    Examples\n    --------\n    &gt;&gt;&gt; conn = db_client.get_connection()\n    &gt;&gt;&gt; conn.closed\n    0\n    \"\"\"\n    return psycopg2.connect(\n        dbname=self.postgres_config.POSTGRES_DATABASE_NAME,\n        user=self.postgres_config.POSTGRES_USER,\n        password=self.postgres_config.POSTGRES_PASSWORD,\n        host=self.postgres_config.POSTGRES_HOST,\n        port=self.postgres_config.POSTGRES_PORT\n    )\n</code></pre>"},{"location":"api/clients.postgress_connector/#src.clients.postgress_connector.PostgreSQLOperation.insert_article","title":"<code>insert_article(article_data)</code>","text":"<p>Insert an article into the database.</p> <p>Uses <code>ON CONFLICT (url) DO NOTHING</code> to avoid duplicates.</p> <p>Parameters:</p> Name Type Description Default <code>article_data</code> <code>dict</code> <p>Dictionary containing keys: 'title', 'author', 'source', 'published_date', 'scraped_date', 'summary', 'content', 'url', 'category'</p> required <p>Raises:</p> Type Description <code>CustomException</code> <p>If insertion fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; article = {\n...     \"title\": \"Test\",\n...     \"author\": \"Alice\",\n...     \"source\": \"BBC\",\n...     \"published_date\": \"2025-09-27T10:00:00Z\",\n...     \"scraped_date\": \"2025-09-27T11:00:00Z\",\n...     \"summary\": \"Test summary\",\n...     \"content\": \"Full content\",\n...     \"url\": \"https://example.com/test\",\n...     \"category\": \"News\"\n... }\n&gt;&gt;&gt; db_client.insert_article(article)\nArticle inserted: https://example.com/test\n</code></pre> Source code in <code>src/clients/postgress_connector.py</code> <pre><code>def insert_article(self, article_data: Dict):\n    \"\"\"\n    Insert an article into the database.\n\n    Uses `ON CONFLICT (url) DO NOTHING` to avoid duplicates.\n\n    Parameters\n    ----------\n    article_data : dict\n        Dictionary containing keys:\n        'title', 'author', 'source', 'published_date', 'scraped_date',\n        'summary', 'content', 'url', 'category'\n\n    Raises\n    ------\n    CustomException\n        If insertion fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; article = {\n    ...     \"title\": \"Test\",\n    ...     \"author\": \"Alice\",\n    ...     \"source\": \"BBC\",\n    ...     \"published_date\": \"2025-09-27T10:00:00Z\",\n    ...     \"scraped_date\": \"2025-09-27T11:00:00Z\",\n    ...     \"summary\": \"Test summary\",\n    ...     \"content\": \"Full content\",\n    ...     \"url\": \"https://example.com/test\",\n    ...     \"category\": \"News\"\n    ... }\n    &gt;&gt;&gt; db_client.insert_article(article)\n    Article inserted: https://example.com/test\n    \"\"\"\n\n    start_time = time.perf_counter()\n    insert_query = sql.SQL(\"\"\"\n    INSERT INTO {table}\n    (title, author, source, publish_date, scraped_date, summary, content, url, category)\n    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n    ON CONFLICT (url) DO NOTHING\n    \"\"\").format(\n            table=sql.Identifier(self.postgres_config.POSTGRES_TABLE_NAME)\n        )\n    try:\n        values = (\n            article_data.get(\"title\"),\n            article_data.get(\"author\"),\n            article_data.get(\"source\"),\n            self._parse_date(article_data.get(\"published_date\")),\n            self._parse_date(article_data.get(\"scraped_date\")),\n            article_data.get(\"summary\"),\n            article_data.get(\"content\"),\n            article_data.get(\"url\"),\n            article_data.get(\"category\"),\n        )\n        with self.connection_cursor() as (_, cursor):\n            cursor.execute(insert_query, values)\n            logger.info(\n                f\"Article inserted: {article_data.get('url')}\",\n                extra={\n                    \"service\": service_name,\n                    \"host\": self.postgres_config.POSTGRES_HOST,\n                    \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n    except Exception as e:\n        logger.error(\n            \"Failed to insert article\",\n            extra={\n                \"service\": service_name,\n                \"host\": self.postgres_config.POSTGRES_HOST,\n                \"stack_trace\": str(e),\n                \"duration_ms\": calculate_duration(start_time),\n            }\n        )\n        raise CustomException(\"Article insert failed\", e)\n</code></pre>"},{"location":"api/clients.postgress_connector/#src.clients.postgress_connector.PostgreSQLOperation.table_exists","title":"<code>table_exists()</code>","text":"<p>Check if the target table exists in the 'public' schema.</p> <p>Returns:</p> Name Type Description <code>exists</code> <code>bool</code> <p>True if table exists, False otherwise.</p> <p>Raises:</p> Type Description <code>CustomException</code> <p>If the check fails due to a DB error.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; db_client.table_exists()\nTrue\n</code></pre> Source code in <code>src/clients/postgress_connector.py</code> <pre><code>def table_exists(self) -&gt; bool:\n    \"\"\"\n    Check if the target table exists in the 'public' schema.\n\n    Returns\n    -------\n    exists : bool\n        True if table exists, False otherwise.\n\n    Raises\n    ------\n    CustomException\n        If the check fails due to a DB error.\n\n    Examples\n    --------\n    &gt;&gt;&gt; db_client.table_exists()\n    True\n    \"\"\"\n    check_query = \"\"\"\n    SELECT EXISTS (\n        SELECT FROM information_schema.tables\n        WHERE table_schema = 'public' AND table_name = %s\n    );\n    \"\"\"\n    start_time = time.perf_counter()\n    with self.connection_cursor() as (_, cursor):\n        try:\n            cursor.execute(check_query, (self.postgres_config.POSTGRES_TABLE_NAME,))\n            exists = cursor.fetchone()[0]\n            logger.info(\n                f\"Table '{self.postgres_config.POSTGRES_TABLE_NAME}' exists: {exists}\",\n                extra={\n                    \"service\": service_name,\n                    \"host\": self.postgres_config.POSTGRES_HOST,\n                    \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n            return exists\n        except Exception as e:\n            logger.error(\n                \"Failed to check if table exists\",\n                extra={\n                    \"service\": service_name,\n                    \"host\": self.postgres_config.POSTGRES_HOST,\n                    \"stack_trace\": str(e),\n                    \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n            raise CustomException(\"Table existence check failed\", e)\n</code></pre>"},{"location":"api/clients.rabbitmq_connector/","title":"RabbitMQ Connector API Reference","text":""},{"location":"api/clients.rabbitmq_connector/#src.clients.rabbitmq_connector.RabbitMQClient","title":"<code>RabbitMQClient</code>","text":"<p>Client for interacting with a RabbitMQ message broker.</p> <p>Handles connection setup, publishing messages, queue declaration, and graceful shutdown with retry logic and error handling.</p> <p>Examples:</p> <p>Basic usage:</p> <pre><code>&gt;&gt;&gt; from src.entity.config_entity import RabbitMQConfig\n&gt;&gt;&gt; from src.clients.rabbitmq_connector import RabbitMQClient\n&gt;&gt;&gt; config = RabbitMQConfig()\n&gt;&gt;&gt; client = RabbitMQClient(config)\n&gt;&gt;&gt; client.declare_queue('my_queue')\n&gt;&gt;&gt; client.publish('my_queue', {\"message\": \"Hello, world!\"})\n&gt;&gt;&gt; client.close()\n</code></pre> <p>Context manager usage:</p> <pre><code>&gt;&gt;&gt; with RabbitMQClient(config) as client:\n...     client.declare_queue('my_queue')\n...     client.publish('my_queue', {\"message\": \"Hello, with context manager!\"})\n</code></pre> Source code in <code>src/clients/rabbitmq_connector.py</code> <pre><code>class RabbitMQClient:\n    \"\"\"\n    Client for interacting with a RabbitMQ message broker.\n\n    Handles connection setup, publishing messages, queue declaration,\n    and graceful shutdown with retry logic and error handling.\n\n    Examples\n    --------\n    Basic usage:\n\n    &gt;&gt;&gt; from src.entity.config_entity import RabbitMQConfig\n    &gt;&gt;&gt; from src.clients.rabbitmq_connector import RabbitMQClient\n    &gt;&gt;&gt; config = RabbitMQConfig()\n    &gt;&gt;&gt; client = RabbitMQClient(config)\n    &gt;&gt;&gt; client.declare_queue('my_queue')\n    &gt;&gt;&gt; client.publish('my_queue', {\"message\": \"Hello, world!\"})\n    &gt;&gt;&gt; client.close()\n\n    Context manager usage:\n\n    &gt;&gt;&gt; with RabbitMQClient(config) as client:\n    ...     client.declare_queue('my_queue')\n    ...     client.publish('my_queue', {\"message\": \"Hello, with context manager!\"})\n    \"\"\"\n\n    def __init__(self, rabbitmq_config: RabbitMQConfig):\n        \"\"\"\n        Initialize RabbitMQ client and establish a connection.\n\n        The connection and channel are created with retry logic.\n\n        Parameters\n        ----------\n        rabbitmq_config : RabbitMQConfig\n            Configuration object with connection parameters.\n\n        Examples\n        --------\n        &gt;&gt;&gt; client = RabbitMQClient(config)\n        &gt;&gt;&gt; assert client.connection.is_open\n        \"\"\"\n        self.rabbitmq_config = rabbitmq_config\n        self.connection: Optional[pika.BlockingConnection] = None\n        self.channel: Optional[pika.adapters.blocking_connection.BlockingChannel] = None\n        self._connect()\n\n\n    def _connect(self):\n        \"\"\"\n        Establish a connection to RabbitMQ with retries and exponential backoff.\n\n        Retries using configuration parameters. Logs connection attempts\n        and raises a `CustomException` on failure.\n\n        Raises\n        ------\n        CustomException\n            If unable to connect after all retry attempts or unexpected errors occur.\n        \"\"\"\n        start_time = time.perf_counter()\n        try:\n            attempt = 0\n            while attempt &lt; self.rabbitmq_config.RETRIES:\n                try:\n                    credentials = pika.PlainCredentials(\n                        username=self.rabbitmq_config.RABBITMQ_USER,\n                        password=self.rabbitmq_config.RABBITMQ_PASSWORD\n                    )\n\n                    parameters = pika.ConnectionParameters(\n                        host=self.rabbitmq_config.RABBITMQ_HOST,\n                        port=self.rabbitmq_config.RABBITMQ_PORT,\n                        virtual_host=self.rabbitmq_config.RABBITMQ_VHOST,\n                        credentials=credentials,\n                        heartbeat=600,\n                        blocked_connection_timeout=300,\n                    )\n\n                    self.connection = pika.BlockingConnection(parameters)\n                    self.channel = self.connection.channel()\n                    logging.info(\n                        \"Connected to RabbitMQ successfully.\",\n                        extra={\n                            \"service\": service_name,\n                            \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                            \"duration_ms\": calculate_duration(start_time),\n                        }\n                    )\n                    break\n\n                except AMQPConnectionError as e:\n                    attempt += 1\n                    logging.warning(\n                        f\"RabbitMQ connection attempt {attempt} failed\",\n                        extra={\n                            \"service\": service_name,\n                            \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                            \"stack_trace\": str(e),\n                            \"duration_ms\": calculate_duration(start_time),\n                        }\n                    )\n                    time.sleep((self.rabbitmq_config.RETRY_BACKOFF_FACTOR ** attempt)  + random.uniform(0, 0.5))  # exponential backoff # nosec B311\n\n                except Exception as e:\n                    logging.error(\n                        \"Unexpected error connecting to RabbitMQ\",\n                        extra={\n                            \"service\": service_name,\n                            \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                            \"stack_trace\": str(e),\n                            \"duration_ms\": calculate_duration(start_time),\n                        }\n                    )\n                    raise\n\n            else:\n                self.close()\n                logging.error(\n                        f\"Failed to connect to RabbitMQ after {attempt} retries.\",\n                        extra={\n                            \"service\": service_name,\n                            \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                            \"duration_ms\": calculate_duration(start_time),\n                        }\n                    )\n                raise ConnectionError(\"Failed to connect to RabbitMQ after retries.\")\n\n        except Exception as e:\n            logging.error(\n                f\"Error in RabbitMQ connection: {e}\",\n                extra={\n                    \"service\": service_name,\n                    \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                    \"stack_trace\": str(e),\n                    \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n            raise CustomException(e, sys)\n\n\n    def declare_queue(self, queue_name: str, durable: bool = True) -&gt; None:\n        \"\"\"\n        Declare a queue on the RabbitMQ server.\n\n        Parameters\n        ----------\n        queue_name : str\n            Name of the queue to declare.\n        durable : bool, optional\n            Whether the queue should survive broker restarts (default True).\n\n        Raises\n        ------\n        CustomException\n            If the queue declaration fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; client.declare_queue('task_queue')\n        \"\"\"\n        start_time = time.perf_counter()\n        try:\n            assert self.channel is not None\n            self.channel.queue_declare(queue=queue_name, durable=durable)\n            logging.info(\n                f\"Declared queue: {queue_name}\",\n                extra={\n                    \"service\": service_name,\n                    \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                    \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n        except Exception as e:\n            logging.error(\n                f\"Failed to declare queue '{queue_name}': {e}\",\n                extra= {\n                    \"service\": service_name,\n                    \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                    \"stack_trace\": str(e),\n                    \"duration_ms\": calculate_duration(start_time),\n                }\n            )\n            raise CustomException(e, sys)\n\n\n    def publish(self, queue_name: str, message: dict) -&gt; None:\n        \"\"\"\n        Publish a message to a specified queue.\n\n        Converts datetime objects in the message to ISO format automatically.\n\n        Parameters\n        ----------\n        queue_name : str\n            Target queue name.\n        message : dict\n            The message body (must be a dictionary).\n\n        Raises\n        ------\n        CustomException\n            If message publishing fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; client.publish('task_queue', {\"task\": \"process_data\"})\n        \"\"\"\n        start_time = time.perf_counter()\n        try:\n            if not isinstance(message, dict):\n                raise CustomException(\"Message must be a dict\", sys)\n            elif isinstance(message.get(\"published\"), datetime):\n                message[\"published\"] = message[\"published\"].isoformat()  # or .strftime(\"%Y-%m-%d\")\n            assert self.channel is not None\n            self.channel.basic_publish(\n                exchange='',\n                routing_key=queue_name,\n                body=json.dumps(message),\n                properties=pika.BasicProperties(\n                    delivery_mode=2  # Make message persistent\n                )\n            )\n            logging.info(\n                f\"Message published: {message}\",\n                extra= {\n                    \"service\": service_name,\n                    \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                    \"duration_ms\": calculate_duration(start_time)\n                }\n\n            )\n        except Exception as e:\n            logging.error(\n                f\"Error publishing message: {e}\",\n                extra= {\n                    \"service\": service_name,\n                    \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                    \"stack_trace\": str(e),\n                    \"duration_ms\": calculate_duration(start_time)\n                }\n            )\n            raise CustomException(e, sys)\n\n\n    def consume(\n            self,\n            queue_name: str,\n            callback: Callable[[pika.channel.Channel, pika.spec.Basic.Deliver, pika.spec.BasicProperties, bytes], None],\n            auto_ack: bool = False,\n            prefetch_count: int = 1\n        ) -&gt; None:\n        \"\"\"\n        Start consuming messages from a queue.\n\n        Parameters\n        ----------\n        queue_name : str\n            Queue to consume from.\n        callback : callable\n            Function to process messages. Signature:\n            callback(channel, method, properties, body)\n        auto_ack : bool, optional\n            Automatically acknowledge messages (default False).\n        prefetch_count : int, optional\n            Number of messages to prefetch (default 1).\n\n        Raises\n        ------\n        CustomException\n            If consuming messages fails.\n        \"\"\"\n        start_time = time.perf_counter()\n        try:\n            assert self.channel is not None\n            self.channel.basic_qos(prefetch_count=prefetch_count)\n            self.channel.basic_consume(queue=queue_name, on_message_callback=callback, auto_ack=auto_ack)\n            logging.info(\n                f\"Started consuming from queue: {queue_name}\",\n                extra= {\n                    \"service\": service_name,\n                    \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                    \"duration_ms\": calculate_duration(start_time)\n                }\n            )\n            self.channel.start_consuming()\n        except KeyboardInterrupt:\n            logging.info(\n                \"Consumer interrupted by user.\",\n                extra= {\n                    \"service\": service_name,\n                    \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                    \"duration_ms\": calculate_duration(start_time)\n                }\n            )\n            self.close()\n            return\n        except ChannelClosedByBroker as e:\n            logging.error(\n                f\"Channel closed by broker: {e}\",\n                extra= {\n                    \"service\": service_name,\n                    \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                    \"stack_trace\": str(e),\n                    \"duration_ms\": calculate_duration(start_time)\n                }\n            )\n            self.close()\n            raise CustomException(e, sys)\n        except Exception as e:\n            logging.error(\n                f\"Error during consuming messages: {e}\",\n                extra= {\n                    \"service\": service_name,\n                    \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                    \"stack_trace\": str(e),\n                    \"duration_ms\": calculate_duration(start_time)\n                }\n            )\n            self.close()\n            raise CustomException(e, sys)\n\n\n    def close(self) -&gt; None:\n        \"\"\"\n        Gracefully close the channel and connection.\n\n        Ensures all resources are released properly. Should be called when\n        RabbitMQ operations are complete.\n\n        Raises\n        ------\n        CustomException\n            If closing the channel or connection fails.\n\n        Examples\n        --------\n        &gt;&gt;&gt; client.close()\n        \"\"\"\n        start_time = time.perf_counter()\n        try:\n            if self.channel and self.channel.is_open:\n                self.channel.close()\n            if self.connection and self.connection.is_open:\n                self.connection.close()\n            logging.info(\n                \"RabbitMQ connection closed.\",\n                extra= {\n                    \"service\": service_name,\n                    \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                    \"duration_ms\": calculate_duration(start_time)\n                }\n            )\n        except Exception as e:\n            logging.warning(\n                f\"Error while closing RabbitMQ connection: {e}\",\n                extra= {\n                    \"service\": service_name,\n                    \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                    \"stack_trace\": str(e),\n                    \"duration_ms\": calculate_duration(start_time)\n                }\n            )\n            raise CustomException(e, sys)\n\n\n    def __enter__(self) -&gt; \"RabbitMQClient\":\n        \"\"\"\n        Enter runtime context for `with` statement.\n\n        Returns\n        -------\n        RabbitMQClient\n            The client instance itself.\n\n        Examples\n        --------\n        &gt;&gt;&gt; with RabbitMQClient(config) as client:\n        ...     client.declare_queue('my_queue')\n        ...     client.publish('my_queue', {\"message\": \"Hello\"})\n        \"\"\"\n        return self\n\n\n    def __exit__(self, exc_type, exc_value, traceback) -&gt; None:\n        \"\"\"\n        Exit runtime context and close the connection.\n\n        Parameters\n        ----------\n        exc_type : type\n            Exception type, if any.\n        exc_value : Exception\n            Exception instance, if any.\n        traceback : traceback\n            Traceback object, if any.\n        \"\"\"\n        self.close()\n</code></pre>"},{"location":"api/clients.rabbitmq_connector/#src.clients.rabbitmq_connector.RabbitMQClient.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter runtime context for <code>with</code> statement.</p> <p>Returns:</p> Type Description <code>RabbitMQClient</code> <p>The client instance itself.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with RabbitMQClient(config) as client:\n...     client.declare_queue('my_queue')\n...     client.publish('my_queue', {\"message\": \"Hello\"})\n</code></pre> Source code in <code>src/clients/rabbitmq_connector.py</code> <pre><code>def __enter__(self) -&gt; \"RabbitMQClient\":\n    \"\"\"\n    Enter runtime context for `with` statement.\n\n    Returns\n    -------\n    RabbitMQClient\n        The client instance itself.\n\n    Examples\n    --------\n    &gt;&gt;&gt; with RabbitMQClient(config) as client:\n    ...     client.declare_queue('my_queue')\n    ...     client.publish('my_queue', {\"message\": \"Hello\"})\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/clients.rabbitmq_connector/#src.clients.rabbitmq_connector.RabbitMQClient.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Exit runtime context and close the connection.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <code>type</code> <p>Exception type, if any.</p> required <code>exc_value</code> <code>Exception</code> <p>Exception instance, if any.</p> required <code>traceback</code> <code>traceback</code> <p>Traceback object, if any.</p> required Source code in <code>src/clients/rabbitmq_connector.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback) -&gt; None:\n    \"\"\"\n    Exit runtime context and close the connection.\n\n    Parameters\n    ----------\n    exc_type : type\n        Exception type, if any.\n    exc_value : Exception\n        Exception instance, if any.\n    traceback : traceback\n        Traceback object, if any.\n    \"\"\"\n    self.close()\n</code></pre>"},{"location":"api/clients.rabbitmq_connector/#src.clients.rabbitmq_connector.RabbitMQClient.__init__","title":"<code>__init__(rabbitmq_config)</code>","text":"<p>Initialize RabbitMQ client and establish a connection.</p> <p>The connection and channel are created with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>rabbitmq_config</code> <code>RabbitMQConfig</code> <p>Configuration object with connection parameters.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = RabbitMQClient(config)\n&gt;&gt;&gt; assert client.connection.is_open\n</code></pre> Source code in <code>src/clients/rabbitmq_connector.py</code> <pre><code>def __init__(self, rabbitmq_config: RabbitMQConfig):\n    \"\"\"\n    Initialize RabbitMQ client and establish a connection.\n\n    The connection and channel are created with retry logic.\n\n    Parameters\n    ----------\n    rabbitmq_config : RabbitMQConfig\n        Configuration object with connection parameters.\n\n    Examples\n    --------\n    &gt;&gt;&gt; client = RabbitMQClient(config)\n    &gt;&gt;&gt; assert client.connection.is_open\n    \"\"\"\n    self.rabbitmq_config = rabbitmq_config\n    self.connection: Optional[pika.BlockingConnection] = None\n    self.channel: Optional[pika.adapters.blocking_connection.BlockingChannel] = None\n    self._connect()\n</code></pre>"},{"location":"api/clients.rabbitmq_connector/#src.clients.rabbitmq_connector.RabbitMQClient.close","title":"<code>close()</code>","text":"<p>Gracefully close the channel and connection.</p> <p>Ensures all resources are released properly. Should be called when RabbitMQ operations are complete.</p> <p>Raises:</p> Type Description <code>CustomException</code> <p>If closing the channel or connection fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client.close()\n</code></pre> Source code in <code>src/clients/rabbitmq_connector.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"\n    Gracefully close the channel and connection.\n\n    Ensures all resources are released properly. Should be called when\n    RabbitMQ operations are complete.\n\n    Raises\n    ------\n    CustomException\n        If closing the channel or connection fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; client.close()\n    \"\"\"\n    start_time = time.perf_counter()\n    try:\n        if self.channel and self.channel.is_open:\n            self.channel.close()\n        if self.connection and self.connection.is_open:\n            self.connection.close()\n        logging.info(\n            \"RabbitMQ connection closed.\",\n            extra= {\n                \"service\": service_name,\n                \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                \"duration_ms\": calculate_duration(start_time)\n            }\n        )\n    except Exception as e:\n        logging.warning(\n            f\"Error while closing RabbitMQ connection: {e}\",\n            extra= {\n                \"service\": service_name,\n                \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                \"stack_trace\": str(e),\n                \"duration_ms\": calculate_duration(start_time)\n            }\n        )\n        raise CustomException(e, sys)\n</code></pre>"},{"location":"api/clients.rabbitmq_connector/#src.clients.rabbitmq_connector.RabbitMQClient.consume","title":"<code>consume(queue_name, callback, auto_ack=False, prefetch_count=1)</code>","text":"<p>Start consuming messages from a queue.</p> <p>Parameters:</p> Name Type Description Default <code>queue_name</code> <code>str</code> <p>Queue to consume from.</p> required <code>callback</code> <code>callable</code> <p>Function to process messages. Signature: callback(channel, method, properties, body)</p> required <code>auto_ack</code> <code>bool</code> <p>Automatically acknowledge messages (default False).</p> <code>False</code> <code>prefetch_count</code> <code>int</code> <p>Number of messages to prefetch (default 1).</p> <code>1</code> <p>Raises:</p> Type Description <code>CustomException</code> <p>If consuming messages fails.</p> Source code in <code>src/clients/rabbitmq_connector.py</code> <pre><code>def consume(\n        self,\n        queue_name: str,\n        callback: Callable[[pika.channel.Channel, pika.spec.Basic.Deliver, pika.spec.BasicProperties, bytes], None],\n        auto_ack: bool = False,\n        prefetch_count: int = 1\n    ) -&gt; None:\n    \"\"\"\n    Start consuming messages from a queue.\n\n    Parameters\n    ----------\n    queue_name : str\n        Queue to consume from.\n    callback : callable\n        Function to process messages. Signature:\n        callback(channel, method, properties, body)\n    auto_ack : bool, optional\n        Automatically acknowledge messages (default False).\n    prefetch_count : int, optional\n        Number of messages to prefetch (default 1).\n\n    Raises\n    ------\n    CustomException\n        If consuming messages fails.\n    \"\"\"\n    start_time = time.perf_counter()\n    try:\n        assert self.channel is not None\n        self.channel.basic_qos(prefetch_count=prefetch_count)\n        self.channel.basic_consume(queue=queue_name, on_message_callback=callback, auto_ack=auto_ack)\n        logging.info(\n            f\"Started consuming from queue: {queue_name}\",\n            extra= {\n                \"service\": service_name,\n                \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                \"duration_ms\": calculate_duration(start_time)\n            }\n        )\n        self.channel.start_consuming()\n    except KeyboardInterrupt:\n        logging.info(\n            \"Consumer interrupted by user.\",\n            extra= {\n                \"service\": service_name,\n                \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                \"duration_ms\": calculate_duration(start_time)\n            }\n        )\n        self.close()\n        return\n    except ChannelClosedByBroker as e:\n        logging.error(\n            f\"Channel closed by broker: {e}\",\n            extra= {\n                \"service\": service_name,\n                \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                \"stack_trace\": str(e),\n                \"duration_ms\": calculate_duration(start_time)\n            }\n        )\n        self.close()\n        raise CustomException(e, sys)\n    except Exception as e:\n        logging.error(\n            f\"Error during consuming messages: {e}\",\n            extra= {\n                \"service\": service_name,\n                \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                \"stack_trace\": str(e),\n                \"duration_ms\": calculate_duration(start_time)\n            }\n        )\n        self.close()\n        raise CustomException(e, sys)\n</code></pre>"},{"location":"api/clients.rabbitmq_connector/#src.clients.rabbitmq_connector.RabbitMQClient.declare_queue","title":"<code>declare_queue(queue_name, durable=True)</code>","text":"<p>Declare a queue on the RabbitMQ server.</p> <p>Parameters:</p> Name Type Description Default <code>queue_name</code> <code>str</code> <p>Name of the queue to declare.</p> required <code>durable</code> <code>bool</code> <p>Whether the queue should survive broker restarts (default True).</p> <code>True</code> <p>Raises:</p> Type Description <code>CustomException</code> <p>If the queue declaration fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client.declare_queue('task_queue')\n</code></pre> Source code in <code>src/clients/rabbitmq_connector.py</code> <pre><code>def declare_queue(self, queue_name: str, durable: bool = True) -&gt; None:\n    \"\"\"\n    Declare a queue on the RabbitMQ server.\n\n    Parameters\n    ----------\n    queue_name : str\n        Name of the queue to declare.\n    durable : bool, optional\n        Whether the queue should survive broker restarts (default True).\n\n    Raises\n    ------\n    CustomException\n        If the queue declaration fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; client.declare_queue('task_queue')\n    \"\"\"\n    start_time = time.perf_counter()\n    try:\n        assert self.channel is not None\n        self.channel.queue_declare(queue=queue_name, durable=durable)\n        logging.info(\n            f\"Declared queue: {queue_name}\",\n            extra={\n                \"service\": service_name,\n                \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                \"duration_ms\": calculate_duration(start_time),\n            }\n        )\n    except Exception as e:\n        logging.error(\n            f\"Failed to declare queue '{queue_name}': {e}\",\n            extra= {\n                \"service\": service_name,\n                \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                \"stack_trace\": str(e),\n                \"duration_ms\": calculate_duration(start_time),\n            }\n        )\n        raise CustomException(e, sys)\n</code></pre>"},{"location":"api/clients.rabbitmq_connector/#src.clients.rabbitmq_connector.RabbitMQClient.publish","title":"<code>publish(queue_name, message)</code>","text":"<p>Publish a message to a specified queue.</p> <p>Converts datetime objects in the message to ISO format automatically.</p> <p>Parameters:</p> Name Type Description Default <code>queue_name</code> <code>str</code> <p>Target queue name.</p> required <code>message</code> <code>dict</code> <p>The message body (must be a dictionary).</p> required <p>Raises:</p> Type Description <code>CustomException</code> <p>If message publishing fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client.publish('task_queue', {\"task\": \"process_data\"})\n</code></pre> Source code in <code>src/clients/rabbitmq_connector.py</code> <pre><code>def publish(self, queue_name: str, message: dict) -&gt; None:\n    \"\"\"\n    Publish a message to a specified queue.\n\n    Converts datetime objects in the message to ISO format automatically.\n\n    Parameters\n    ----------\n    queue_name : str\n        Target queue name.\n    message : dict\n        The message body (must be a dictionary).\n\n    Raises\n    ------\n    CustomException\n        If message publishing fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; client.publish('task_queue', {\"task\": \"process_data\"})\n    \"\"\"\n    start_time = time.perf_counter()\n    try:\n        if not isinstance(message, dict):\n            raise CustomException(\"Message must be a dict\", sys)\n        elif isinstance(message.get(\"published\"), datetime):\n            message[\"published\"] = message[\"published\"].isoformat()  # or .strftime(\"%Y-%m-%d\")\n        assert self.channel is not None\n        self.channel.basic_publish(\n            exchange='',\n            routing_key=queue_name,\n            body=json.dumps(message),\n            properties=pika.BasicProperties(\n                delivery_mode=2  # Make message persistent\n            )\n        )\n        logging.info(\n            f\"Message published: {message}\",\n            extra= {\n                \"service\": service_name,\n                \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                \"duration_ms\": calculate_duration(start_time)\n            }\n\n        )\n    except Exception as e:\n        logging.error(\n            f\"Error publishing message: {e}\",\n            extra= {\n                \"service\": service_name,\n                \"host\": self.rabbitmq_config.RABBITMQ_HOST,\n                \"stack_trace\": str(e),\n                \"duration_ms\": calculate_duration(start_time)\n            }\n        )\n        raise CustomException(e, sys)\n</code></pre>"},{"location":"architecture/components_overview/","title":"Components Overview","text":"<p>This page provides a technical breakdown of each component in the BBC News ETL pipeline.</p>"},{"location":"architecture/components_overview/#components","title":"Components","text":"<ul> <li> <p>Primary Producer &amp; Scrapers: Generate work queue, scrape BBC News articles, deduplicate, and push tasks to RabbitMQ.</p> </li> <li> <p>Message Queue (RabbitMQ): Manages Work Queue, Task Queue, and DLQ for decoupling and reliability.</p> </li> <li> <p>Consumers (ETL Workers): Fetch tasks, parse and transform raw HTML into structured datasets, store in MongoDB &amp; PostgreSQL, and handle DLQ messages.</p> </li> <li> <p>Storage Layer:</p> <ul> <li>MongoDB: raw/unstructured HTML.</li> <li>PostgreSQL: cleaned, structured analytics-ready data.</li> </ul> </li> <li> <p>Monitoring &amp; Observability: Prometheus metrics, Grafana dashboards, Loki logs (via Promtail).</p> </li> <li> <p>Deployment &amp; Orchestration: Docker, Kubernetes + KEDA for autoscaling, Helm charts, CI/CD pipelines.</p> </li> </ul>"},{"location":"architecture/components_overview/#1-producer-scraper","title":"1. Producer (Scraper)","text":"<ul> <li> <p>Purpose: Generate work queue, scrape live and archived news articles, deduplicate against MongoDB, and publish tasks to RabbitMQ.</p> </li> <li> <p>Implementation:</p> <ul> <li>Python 3.11 + <code>requests</code>, <code>BeautifulSoup4</code>, <code>Selenium</code>, <code>pika</code>.</li> <li>Handles rate limiting, retries, and error logging via Promtail.</li> </ul> </li> <li> <p>Key Features:</p> <ul> <li>Modular scrapers supporting multiple sections.</li> <li>Work Queue orchestration based on dates and statistical thresholds.</li> <li>Horizontally scalable via KEDA.</li> </ul> </li> </ul>"},{"location":"architecture/components_overview/#2-message-queue-rabbitmq","title":"2. Message Queue (RabbitMQ)","text":"<ul> <li> <p>Purpose: Decouple producers and consumers while ensuring reliable task delivery.</p> </li> <li> <p>Implementation:</p> <ul> <li>Maintains Work Queue, Task Queue, DLQ.</li> <li>Durable queues with prefetch, acknowledgements, retries.</li> </ul> </li> <li> <p>Key Features:</p> <ul> <li>Supports multiple producers and consumers.</li> <li>Fault-tolerant messaging with DLQ for failed ETL tasks.</li> <li>Prometheus metrics for queue depth, throughput, and failures.</li> </ul> </li> </ul>"},{"location":"architecture/components_overview/#3-consumer-etl-worker","title":"3. Consumer (ETL Worker)","text":"<ul> <li> <p>Purpose: Transform raw HTML into structured datasets and load them into MongoDB/PostgreSQL.</p> </li> <li> <p>Implementation:</p> <ul> <li>Python ETL scripts + <code>pandas</code>, <code>sqlalchemy</code>, <code>pymongo</code>.</li> <li>Retry and DLQ integration for failed messages.</li> </ul> </li> <li> <p>Key Features:</p> <ul> <li>Parallel and scalable processing via KEDA.</li> <li>Metrics exposed to Prometheus; logs sent via Promtail \u2192 Loki.</li> <li>Handles failed ETL tasks with DLQ for manual inspection.</li> </ul> </li> </ul>"},{"location":"architecture/components_overview/#4-storage-layer","title":"4. Storage Layer","text":"<ul> <li> <p>MongoDB: Raw/unstructured HTML storage (data lake).</p> </li> <li> <p>PostgreSQL: Cleaned, analytics-ready datasets.</p> </li> <li> <p>Features:</p> <ul> <li>Versioned datasets for reproducibility.</li> <li>Optimized for query performance and downstream analytics.</li> </ul> </li> </ul>"},{"location":"architecture/components_overview/#5-observability","title":"5. Observability","text":"<ul> <li>Prometheus: Collects metrics (queue depth, processing rates, error counts).</li> <li>Grafana: Dashboards visualize metrics and pipeline performance.</li> <li> <p>Loki + Promtail: Centralized logging from all components for errors, retries, and debugging.</p> </li> <li> <p>Features:</p> <ul> <li>Alerts for failures, bottlenecks, or anomalies.</li> <li>Real-time monitoring for production-grade deployments.</li> </ul> </li> </ul>"},{"location":"architecture/components_overview/#6-deployment-orchestration","title":"6. Deployment &amp; Orchestration","text":"<ul> <li>Docker: Containerizes all components.</li> <li>Kubernetes + KEDA: Autoscaling, rolling updates, health checks.</li> <li>Helm charts: Simplify environment configuration and deployment.</li> <li>CI/CD: GitHub Actions for testing and automated deployment.</li> </ul>"},{"location":"architecture/high_level_data_flow/","title":"High-Level Data Flow","text":"<p>This section provides an end-to-end view of the BBC News ETL pipeline, highlighting the flow of data from scraping to storage, monitoring, and failure handling.</p>"},{"location":"architecture/high_level_data_flow/#overview","title":"Overview","text":"<p>The pipeline is designed to ingest news data reliably and at scale. It follows a modular architecture with clearly separated concerns:</p> <ol> <li> <p>Primary Producer &amp; Work Queue</p> <ul> <li>The primary producer generates a date-based work queue based on configured <code>START_DATE</code> and <code>CURRENT_DATE</code>.</li> <li>Checks MongoDB (data lake) for existing records and ensures minimum article count per date.</li> <li>Pending dates are added to the Work Queue for scraping.</li> </ul> </li> <li> <p>Data Producers (Scrapers)</p> <ul> <li>Multiple producers (scaled by KEDA) pick dates from the Work Queue.</li> <li>Scrape live and archived BBC News articles for assigned dates.</li> <li>Perform duplicate checks against MongoDB before publishing.</li> <li>Publish unique article links to the Task Queue (RabbitMQ).</li> </ul> </li> <li> <p>Message Queue (RabbitMQ)</p> <ul> <li>Decouples producers and consumers.</li> <li>Maintains Work Queue, Task Queue, and Dead Letter Queue (DLQ) for failed ETL tasks.</li> <li>Supports durable queues, acknowledgements, and retries.</li> </ul> </li> <li> <p>Consumers / ETL Workers</p> <ul> <li>Fetch messages from the Task Queue.</li> <li>Parse raw HTML into structured, analytics-ready data.</li> <li>Apply transformations (cleaning, normalization, enrichment).</li> <li> <p>Store results in:</p> <ul> <li>MongoDB: raw/unstructured HTML content.</li> <li>PostgreSQL: cleaned, structured datasets.</li> </ul> </li> <li> <p>Failed ETL messages are routed to DLQ.</p> </li> </ul> </li> <li> <p>Observability &amp; Monitoring</p> <ul> <li>Prometheus collects metrics from producers, consumers, and RabbitMQ.</li> <li>Grafana visualizes pipeline performance via dashboards.</li> <li>Logs from all components are sent via Promtail \u2192 Loki for centralized analysis.</li> </ul> </li> <li> <p>Deployment &amp; Orchestration</p> <ul> <li>All components are containerized.</li> <li>Kubernetes + KEDA handles autoscaling, health checks, and rolling updates.</li> <li>Helm charts simplify deployment, configuration, and environment management.</li> </ul> </li> </ol>"},{"location":"architecture/high_level_data_flow/#data-flow-diagram","title":"Data Flow Diagram","text":"<pre><code>flowchart TD\n    PrimaryProducer[Primary Producer] --&gt; WorkQueue[\"Work Queue (Dates)\"]\n    WorkQueue --&gt; MultiProducers[\"Multiple Producers\"]\n    MultiProducers --&gt; TaskQueue[\"Task Queue (Article Links)\"]\n    TaskQueue --&gt; ConsumersNode[\"Multiple Consumers\"]\n\n    %% Storage\n    ConsumersNode --&gt; MongoDB[\"MongoDB Raw Storage\"]\n    ConsumersNode --&gt; Postgres[\"PostgreSQL Clean Storage\"]\n    ConsumersNode --&gt;|Fail| DLQ[\"Dead Letter Queue\"]\n\n    %% Metrics\n    MultiProducers --&gt; MetricsNode[\"Prometheus Metrics\"]\n    ConsumersNode --&gt; MetricsNode\n    RabbitMQNode[(RabbitMQ)] --&gt; MetricsNode\n    MetricsNode --&gt; Grafana[\"Grafana Dashboards\"]\n\n    %% Logs\n    MultiProducers --&gt; LoggerNode[\"Logs\"]\n    ConsumersNode --&gt; LoggerNode\n    LoggerNode --&gt; Promtail[\"Promtail Agent\"]\n    Promtail --&gt; Loki[\"Loki\"]</code></pre> <p>This updated diagram and overview now reflect:</p> <ul> <li>Work Queue \u2192 Multiple Producers \u2192 Task Queue \u2192 Consumers</li> <li>DLQ for failed ETL tasks</li> <li>KEDA scaling for Producers &amp; Consumers</li> <li>Promtail \u2192 Loki for centralized logging</li> <li>Prometheus \u2192 Grafana for metrics dashboards</li> </ul>"},{"location":"cicd/","title":"CI/CD &amp; Infrastructure","text":"<p>This project is built with a cloud-native infrastructure mindset and supports continuous integration, continuous delivery (CI/CD), and reproducible infrastructure provisioning.</p>"},{"location":"cicd/#1-cicd-pipeline","title":"1. CI/CD Pipeline","text":""},{"location":"cicd/#github-actions","title":"GitHub Actions","text":"<ul> <li>Automated Builds:   Each commit triggers Docker image builds for Producers, Consumers, and supporting services.</li> <li>Linting &amp; Testing:   Unit tests, linting (<code>flake8</code>, <code>black</code>), and integration tests run on every PR.</li> <li>Documentation Deployment:   MkDocs site is automatically built and deployed via GitHub Pages.</li> <li>Security Scans:   Dependencies are checked for vulnerabilities using <code>pip-audit</code>.</li> </ul>"},{"location":"cicd/#jenkins-optional-extension","title":"Jenkins (Optional Extension)","text":"<p>For enterprise environments, Jenkins can be integrated to:</p> <ul> <li>Run end-to-end ETL jobs in a staging environment.</li> <li>Deploy to Kubernetes clusters after passing automated QA gates.</li> <li>Support manual approvals for production rollout.</li> </ul>"},{"location":"cicd/#2-infrastructure-as-code-iac","title":"2. Infrastructure as Code (IaC)","text":"<p>Infrastructure is defined declaratively using Terraform and Helm:</p>"},{"location":"cicd/#terraform","title":"Terraform","text":"<ul> <li>Provisions cloud resources (VMs, storage, networking, managed DBs).</li> <li>Ensures reproducibility across environments (dev, staging, prod).</li> <li>Example: Creates an EKS cluster (AWS) or GKE cluster (GCP).</li> </ul>"},{"location":"cicd/#helm","title":"Helm","text":"<ul> <li>Used to package and deploy Producers, Consumers, RabbitMQ, Prometheus, Grafana, and Loki.</li> <li>Supports environment-specific values (e.g., dev vs prod scaling).</li> <li>Simplifies rolling upgrades and rollback.</li> </ul>"},{"location":"cicd/#3-deployment-workflow","title":"3. Deployment Workflow","text":"<ol> <li>Developer pushes code \u2192 GitHub Actions runs tests &amp; builds Docker images.</li> <li>Images are pushed to a container registry (e.g., AWS ECR, DockerHub, or GCP Artifact Registry).</li> <li>Helm + Terraform deploy the updated components to Kubernetes.</li> <li>KEDA + Prometheus + Loki ensure auto-scaling and observability.</li> <li>Grafana dashboards &amp; alerts notify if issues arise post-deployment.</li> </ol>"},{"location":"cicd/#4-reliability-in-deployment","title":"4. Reliability in Deployment","text":"<ul> <li>Blue-Green Deployments: New pods are tested before switching traffic.</li> <li>Rollback Mechanism: Helm allows rollback to a previous release if deployment fails.</li> <li>Immutable Images: Each release is tied to a Git commit SHA for traceability.</li> <li>Secrets Management: Handled via Kubernetes Secrets or Vault integration.</li> </ul>"},{"location":"cicd/#5-cicd-pipeline-diagram","title":"5. CI/CD Pipeline Diagram","text":"<pre><code>flowchart TD\n    Dev[Developer Push Code] --&gt; GH[GitHub Actions]\n    GH --&gt; Tests[Linting &amp; Tests]\n    GH --&gt; Build[Build Docker Images]\n    Build --&gt; Registry[Container Registry]\n    Registry --&gt; Helm[Helm Deploy]\n    Helm --&gt; K8s[Kubernetes Cluster]\n    K8s --&gt; KEDA[Autoscaling with KEDA]\n    K8s --&gt; Prometheus[Metrics]\n    K8s --&gt; Loki[Logs]\n    Prometheus --&gt; Grafana[Dashboards &amp; Alerts]</code></pre>"},{"location":"cicd/#key-highlights","title":"Key Highlights","text":"<ul> <li>Automated CI/CD with GitHub Actions &amp; optional Jenkins.</li> <li>IaC with Terraform + Helm ensures reproducible environments.</li> <li>Scalable Kubernetes-native deployment.</li> <li>Built-in reliability with blue-green deployment and rollback.</li> </ul>"},{"location":"components/consumer/","title":"Consumer (ETL Worker)","text":"<p>Consumers process task messages from RabbitMQ, perform ETL, and store results in MongoDB (raw) and PostgreSQL (cleaned). They also handle failed ETL tasks via a Dead Letter Queue (DLQ) and expose metrics/logs for observability.</p>"},{"location":"components/consumer/#responsibilities","title":"Responsibilities","text":"<ul> <li>Fetch article link messages from the Task Queue (RabbitMQ).</li> <li>Parse raw HTML into structured, analytics-ready records.</li> <li>Apply cleaning, normalization, and enrichment.</li> <li>Store:<ul> <li>MongoDB \u2192 raw data (data lake)</li> <li>PostgreSQL \u2192 cleaned &amp; analytics ready data (data warehouse)</li> </ul> </li> <li> <p>Handle failed ETL tasks:</p> <ul> <li>Messages that fail ETL are routed to DLQ for manual inspection.</li> </ul> </li> <li> <p>Emit metrics and logs for observability:</p> <ul> <li>Metrics \u2192 Prometheus</li> <li>Logs \u2192 Promtail \u2192 Loki</li> </ul> </li> </ul>"},{"location":"components/consumer/#implementation","title":"Implementation","text":"<ul> <li>Language: Python 3.11</li> <li>Libraries: <code>pika</code>, <code>pandas</code>, <code>sqlalchemy</code>, <code>pymongo</code></li> <li> <p>Features:</p> </li> <li> <p>Parallel processing of messages for high throughput.</p> </li> <li>Retry &amp; DLQ integration for fault tolerance.</li> <li>Metrics exposed to Prometheus.</li> <li>Logs centralized via Promtail \u2192 Loki.</li> <li>Fully containerized for Kubernetes deployment.</li> </ul>"},{"location":"components/consumer/#data-flow","title":"Data Flow","text":"<pre><code>flowchart TD\n    TaskQueue[\"Task Queue (Article Links)\"] --&gt; ConsumersNode\n    ConsumersNode[\"Multiple Consumers\"] --&gt; MongoDB[\"Raw Storage\"]\n    ConsumersNode --&gt; Postgres[\"Clean Storage\"]\n    ConsumersNode --&gt;|Fail| DLQ[\"Dead Letter Queue\"]\n    ConsumersNode --&gt; MetricsNode[\"Prometheus Metrics\"]\n    ConsumersNode --&gt; LoggerNode[\"Logs\"]\n    LoggerNode --&gt; Promtail[\"Promtail Agent\"]\n    Promtail --&gt; Loki[\"Loki\"]</code></pre>"},{"location":"components/consumer/#key-highlights","title":"Key Highlights","text":"<ul> <li>Parallel &amp; scalable: KEDA spawns multiple Consumer pods based on Task Queue depth.</li> <li>Reliability: Failed ETL messages are captured in DLQ for manual recovery.</li> <li>Observability: Metrics + logs provide real-time insights into processing rates, failures, and ETL throughput.</li> <li>Production-ready: Fully containerized and ready for Kubernetes deployment.</li> </ul>"},{"location":"components/monitoring/","title":"Monitoring &amp; Observability (Prometheus + Grafana + Loki)","text":"<p>This component ensures full observability of the ETL pipeline, including metrics, logs, and dashboards for producers, consumers, and RabbitMQ.</p>"},{"location":"components/monitoring/#components","title":"Components","text":""},{"location":"components/monitoring/#prometheus","title":"Prometheus","text":"<ul> <li> <p>Collects metrics from all pipeline components:</p> <ul> <li>Producers \u2192 scraped links, retries, work queue length</li> <li>Consumers \u2192 ETL throughput, failed tasks</li> <li>RabbitMQ \u2192 queue depth, message rates</li> <li>Enables alerts for failures, slow processing, or resource bottlenecks.</li> </ul> </li> </ul>"},{"location":"components/monitoring/#grafana","title":"Grafana","text":"<ul> <li>Visualizes metrics via customizable dashboards.</li> <li>Provides real-time insights into system health and scaling behavior.</li> <li> <p>Examples of dashboards:</p> <ul> <li>Queue depth trends</li> <li>Producer &amp; Consumer throughput</li> <li>DLQ failure rates</li> </ul> </li> </ul>"},{"location":"components/monitoring/#loki-promtail","title":"Loki &amp; Promtail","text":"<ul> <li>Producers and Consumers send logs to stdout/file.</li> <li>Promtail Agent collects and forwards logs to Loki.</li> <li> <p>Centralized logging includes:</p> <ul> <li>Errors, retries, DLQ messages</li> <li>Debug information for troubleshooting</li> <li>Supports querying, filtering, and dashboards in Grafana.</li> </ul> </li> </ul>"},{"location":"components/monitoring/#data-flow","title":"Data Flow","text":"<pre><code>flowchart TD\n    ProducerInit --&gt; WorkQueue\n    WorkQueue --&gt; RabbitMQNode\n    RabbitMQNode --&gt; MultiProducers\n    MultiProducers --&gt; TaskQueue\n    TaskQueue --&gt; ConsumersNode\n    ConsumersNode --&gt; MongoDB[Raw Storage]\n    ConsumersNode --&gt; Postgres[Clean Storage]\n    ConsumersNode --&gt;|Fail| DLQ[Dead Letter Queue]\n\n    %% Metrics\n    MultiProducers --&gt; MetricsNode[Prometheus]\n    ConsumersNode --&gt; MetricsNode\n    RabbitMQNode --&gt; MetricsNode\n    MetricsNode --&gt; Grafana[Grafana Dashboards]\n\n    %% Logs\n    MultiProducers --&gt; LoggerNode[Logs]\n    ConsumersNode --&gt; LoggerNode\n    LoggerNode --&gt; Promtail[Promtail Agent]\n    Promtail --&gt; Loki[Loki]</code></pre>"},{"location":"components/monitoring/#key-highlights","title":"Key Highlights","text":"<ul> <li>Centralized observability: metrics and logs from all components are aggregated.</li> <li>Real-time monitoring &amp; alerting: track producer/consumer performance, queue depth, DLQ failures.</li> <li>Kubernetes-friendly: integrates seamlessly with containerized deployments and KEDA scaling.</li> <li>Production-grade: provides full visibility for debugging, scaling, and SLA monitoring.</li> </ul>"},{"location":"components/producer/","title":"Producer (Scraper &amp; Orchestrator)","text":"<p>The Producer is responsible for orchestrating scraping tasks, generating work queues, and publishing unique BBC News article links into RabbitMQ for downstream ETL processing. It ensures deduplication, autoscaling, and observability.</p>"},{"location":"components/producer/#responsibilities","title":"Responsibilities","text":"<ul> <li> <p>Work Queue Generation:</p> <ul> <li>On start, the primary producer generates a date-based work queue.</li> <li> <p>Uses configuration values:</p> <ul> <li><code>START_DATE</code> \u2192 beginning of the scraping window.</li> <li><code>CURRENT_DATE</code> \u2192 dynamically calculated at runtime.</li> </ul> </li> <li> <p>For each date in the range:</p> <ul> <li>Checks MongoDB (data lake) for existing records.</li> <li>If no records found \u2192 marks the date as a work date.</li> <li>If record count &lt; statistical threshold (configurable, e.g., average articles/day) \u2192 also mark the date as a work date to ensure data completeness.</li> </ul> </li> <li> <p>Populates the Work Queue with all pending dates.</p> </li> </ul> </li> <li> <p>Task Distribution:</p> <ul> <li>KEDA scales multiple producers to pick up dates from the Work Queue.</li> <li>Each producer scrapes all article links for its assigned date.</li> <li>Publishes only new and unique links to the Task Queue (RabbitMQ).</li> </ul> </li> <li> <p>Message Publishing:</p> <ul> <li> <p>Each task message contains:</p> <ul> <li>Date</li> <li>Article URL</li> <li>Basic metadata<ul> <li>Sent to RabbitMQ for consumers.</li> <li>Failed ETL tasks are routed to a Dead Letter Queue (DLQ) for manual recovery.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Observability:</p> <ul> <li>Logs \u2192 collected by Promtail \u2192 Loki.</li> <li>Metrics \u2192 exposed for Prometheus (e.g., scraped links count, retries, duplicates skipped).</li> </ul> </li> </ul>"},{"location":"components/producer/#implementation","title":"Implementation","text":"<ul> <li>Language: Python 3.11</li> <li> <p>Libraries:</p> <ul> <li>Scraping: <code>requests</code>, <code>BeautifulSoup4</code>, <code>Selenium</code></li> <li>Messaging: <code>pika</code> (RabbitMQ client)</li> <li>Storage: <code>pymongo</code> (MongoDB connector)</li> </ul> </li> <li> <p>Features:</p> <ul> <li>Date-based work queue generator.</li> <li>Duplicate check against MongoDB.</li> <li>Modular scraper with support for multiple BBC sections.</li> <li>Error handling, retry logic, DLQ routing.</li> <li>Configurable via environment variables.</li> </ul> </li> </ul>"},{"location":"components/producer/#data-flow","title":"Data Flow","text":"<pre><code>flowchart TD\n    Config[Config: START_DATE, CURRENT_DATE] --&gt; WorkGen[Generate Work Queue]\n    WorkGen --&gt;|Pending Dates| KEDA\n    KEDA --&gt; MultiProd[Multiple Producers]\n\n    MultiProd --&gt;|Scraped Links| DedupCheck[Check in MongoDB]\n    DedupCheck --&gt;|Unique| RabbitMQ[Task Queue]\n    DedupCheck --&gt;|Duplicate| Skip[Skip Publish]\n\n    RabbitMQ --&gt; Consumer[ETL Worker]\n\n    Consumer --&gt;|Success| Postgres[(Postgres)]\n    Consumer --&gt;|Fail| DLQ[Dead Letter Queue]\n\n    MultiProd --&gt; Metrics[Prometheus Metrics]\n    MultiProd --&gt; Logger[Logs]\n    Logger --&gt; Promtail\n    Promtail --&gt; Loki</code></pre>"},{"location":"components/producer/#key-highlights","title":"Key Highlights","text":"<ul> <li>Orchestrator Role: First producer creates work queue; others scale dynamically.</li> <li>Duplicate Prevention: MongoDB check ensures no redundant messages.</li> <li>Autoscaling: KEDA spawns Producers &amp; Consumers based on queue length.</li> <li>Failure Handling: DLQ captures failed ETL tasks for manual inspection.</li> <li>Production-grade Observability: Logs \u2192 Loki, Metrics \u2192 Prometheus.</li> </ul>"},{"location":"components/rabbitmq/","title":"Message Queue (RabbitMQ)","text":"<p>The Message Queue acts as the backbone of the pipeline, decoupling producers from consumers and ensuring reliable, ordered, and durable delivery of scraping and ETL tasks.</p>"},{"location":"components/rabbitmq/#responsibilities","title":"Responsibilities","text":"<ul> <li> <p>Maintain Work Queue for Producers:</p> <ul> <li>Stores date-based tasks generated by the primary producer.</li> <li>Enables multiple Producers (scaled by KEDA) to pick pending dates and scrape them.</li> </ul> </li> <li> <p>Maintain Task Queue for Consumers:</p> <ul> <li>Stores scraped article links + metadata.</li> <li>Allows multiple Consumers to process tasks in parallel for ETL into storage systems.</li> </ul> </li> <li> <p>Provide Dead Letter Queue (DLQ):</p> <ul> <li>Captures failed ETL messages for manual inspection and replay.</li> </ul> </li> <li> <p>Ensure    -</p> <ul> <li>Durable storage of messages (no data loss).</li> <li>Acknowledgements for reliable message processing.</li> <li>Retry &amp; requeue support for transient failures.</li> <li>M-trics integration for observability.</li> </ul> </li> </ul>"},{"location":"components/rabbitmq/#implementation","title":"Implementation","text":"<ul> <li> <p>RabbitMQ with:</p> <ul> <li>Durable queues (Work Queue, Task Queue, DLQ).</li> <li>Prefetch settings to prevent consumer overload.</li> <li>Acknowledgements for guaranteed delivery.</li> <li>Integrated with Prometheus RabbitMQ Exporter for queue-level metrics.</li> </ul> </li> </ul>"},{"location":"components/rabbitmq/#data-flow","title":"Data Flow","text":"<pre><code>flowchart TD\n    ProducerInit --&gt; WorkQueue\n    WorkQueue[Work Queue] --&gt; RabbitMQNode\n    RabbitMQNode[(RabbitMQ)] --&gt; MultiProducers\n    MultiProducers[Multiple Producers] --&gt; TaskQueue\n    TaskQueue[Task Queue] --&gt; RabbitMQNode\n    RabbitMQNode --&gt; ConsumersNode\n    ConsumersNode[Consumers - ETL Workers] --&gt; DLQ\n    DLQ[Dead Letter Queue]\n    RabbitMQNode --&gt; MetricsNode\n    MetricsNode[Prometheus Exporter]</code></pre>"},{"location":"components/rabbitmq/#key-highlights","title":"Key Highlights","text":"<ul> <li> <p>Two-tier queueing system:</p> <ul> <li>Work Queue (date orchestration) \u2192 drives Producers.</li> <li>Task Queue (article links) \u2192 drives Consumers.</li> </ul> </li> <li> <p>Autoscaling with KEDA: queue depth automatically adjusts number of Producers and Consumers.</p> </li> <li> <p>Reliability: DLQ captures all failures for manual recovery.</p> </li> <li> <p>Observability: Queue depth, processing rates, and failures are exposed as Prometheus metrics.</p> </li> </ul>"},{"location":"components/storage/","title":"Storage Layer (MongoDB + PostgreSQL)","text":"<p>This layer stores both raw and processed news data.</p>"},{"location":"components/storage/#components","title":"Components","text":""},{"location":"components/storage/#mongodb-raw-storage","title":"MongoDB (Raw Storage)","text":"<ul> <li>Stores unprocessed HTML and metadata.</li> <li>Schema-less design allows flexible storage of raw articles.</li> <li>Useful for auditing and reprocessing pipelines.</li> </ul>"},{"location":"components/storage/#postgresql-clean-storage","title":"PostgreSQL (Clean Storage)","text":"<ul> <li>Stores structured, analytics-ready data.</li> <li>Supports indexes and optimized queries for downstream analytics.</li> <li>Versioned datasets for reproducibility.</li> </ul>"},{"location":"components/storage/#data-flow","title":"Data Flow","text":"<pre><code>flowchart LR\n    Consumer --&gt; MongoDB[Raw Storage]\n    Consumer --&gt; Postgres[Clean Storage]</code></pre> <p>Key Highlights</p> <ul> <li>Separation of raw vs cleaned data ensures reproducibility.</li> <li>Optimized for analytics and reporting in PostgreSQL.</li> <li>Centralized backups for reliability and disaster recovery.</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>The BBC News ETL Pipeline is highly configurable via environment variables, configuration files, and Helm values. This page documents all the key configuration options for producers, consumers, message queues, storage, and observability.</p>"},{"location":"configuration/#1-environment-variables","title":"1. Environment Variables","text":"<p>Environment variables allow you to customize behavior without modifying code. Some key variables include:</p>"},{"location":"configuration/#producer-scraper","title":"Producer / Scraper","text":"Variable Description Default <code>START_DATE</code> Start date for scraping (YYYY-MM-DD) <code>2025-01-01</code> <code>CURRENT_DATE</code> Current date for work queue generation System date <code>SCRAPE_INTERVAL</code> Interval between scraping tasks (seconds) <code>10</code> <code>SELENIUM_URL</code> URL to connect to Selenium WebDriver <code>http://selenium:4444/wd/hub</code> <code>RABBITMQ_HOST</code> RabbitMQ hostname <code>rabbitmq</code> <code>RABBITMQ_QUEUE</code> Task queue name <code>task_queue</code> <code>RABBITMQ_DLQ</code> Dead Letter Queue name <code>dlq_queue</code>"},{"location":"configuration/#consumer-etl-worker","title":"Consumer / ETL Worker","text":"Variable Description Default <code>RABBITMQ_HOST</code> RabbitMQ hostname <code>rabbitmq</code> <code>TASK_QUEUE</code> Name of task queue to consume <code>task_queue</code> <code>DLQ_QUEUE</code> Name of DLQ <code>dlq_queue</code> <code>MONGO_URI</code> MongoDB connection string <code>mongodb://mongo:27017</code> <code>POSTGRES_URI</code> PostgreSQL connection string <code>postgresql://postgres:5432/news</code> <code>MAX_RETRIES</code> Number of retries before sending to DLQ <code>3</code>"},{"location":"configuration/#observability","title":"Observability","text":"Variable Description Default <code>PROMETHEUS_ENDPOINT</code> URL for Prometheus metrics <code>/metrics</code> <code>LOKI_URL</code> Loki endpoint for logs <code>http://loki:3100/loki/api/v1/push</code> <code>LOG_LEVEL</code> Logging level <code>INFO</code>"},{"location":"configuration/#2-configuration-files","title":"2. Configuration Files","text":"<p>Some components support YAML/JSON configuration files for advanced settings:</p> <ul> <li><code>config/producers.yaml</code>   Configure sections to scrape, concurrency, and retries.</li> <li><code>config/consumers.yaml</code>   Define ETL transformation rules, batch size, and retry policies.</li> <li><code>config/helm/values.yaml</code>   Customize deployment settings for Kubernetes (resource limits, replicas, KEDA scaling, secrets).</li> </ul>"},{"location":"configuration/#3-helm-chart-configuration","title":"3. Helm Chart Configuration","text":"<p>For Kubernetes deployments:</p> <ul> <li>Replicas: Scale producers and consumers based on queue depth using KEDA.</li> <li>Resources: Set CPU/memory limits and requests per pod.</li> <li>Secrets: Store database credentials, RabbitMQ credentials, and API keys securely.</li> <li>Metrics &amp; Logging: Configure Prometheus scraping, Grafana dashboards, and Loki endpoints.</li> </ul> <p>Example snippet from <code>values.yaml</code>:</p> <pre><code>producers:\n  replicas: 2\n  seleniumUrl: http://selenium:4444/wd/hub\n  startDate: 2025-01-01\n\nconsumers:\n  replicas: 2\n  maxRetries: 3\n\nrabbitmq:\n  host: rabbitmq\n  taskQueue: task_queue\n  dlqQueue: dlq_queue\n</code></pre>"},{"location":"configuration/#4-notes-best-practices","title":"4. Notes &amp; Best Practices","text":"<ul> <li>Version control: Keep <code>config/*.yaml</code> files under Git for reproducibility.</li> <li>Environment separation: Use different values for local, staging, and production environments.</li> <li>Secrets: Never hardcode passwords or API keys; use environment variables or Kubernetes secrets.</li> <li>Dynamic scaling: Configure KEDA triggers carefully to avoid over/under-provisioning producers and consumers.</li> </ul>"},{"location":"deployment/docker/","title":"Docker Deployment","text":"<p>This guide describes how to run the BBC News ETL pipeline locally using Docker Compose. Each component is containerized for portability and reproducibility.</p>"},{"location":"deployment/docker/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker &gt;= 24.x</li> <li>Docker Compose &gt;= 2.x</li> <li>Python 3.11 (for local development, optional if using pre-built images)</li> </ul>"},{"location":"deployment/docker/#services-overview","title":"Services Overview","text":"<p>The Docker Compose stack includes:</p> <ul> <li>Primary Producer \u2192 generates work queue</li> <li>Producers \u2192 scrape articles and push tasks</li> <li>RabbitMQ \u2192 message queue (Task Queue &amp; DLQ)</li> <li>Consumers (ETL Workers) \u2192 fetch and transform messages</li> <li>MongoDB \u2192 raw data storage</li> <li>PostgreSQL \u2192 cleaned data storage</li> <li>Prometheus \u2192 metrics collection</li> <li>Grafana \u2192 dashboards</li> <li>Promtail \u2192 Loki \u2192 centralized logging</li> </ul>"},{"location":"deployment/docker/#getting-started","title":"Getting Started","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/Rahul-404/bbc_news_etl_pipeline.git\ncd bbc_news_etl_pipeline\n</code></pre> <ol> <li>Build Docker images (or pull pre-built images from registry):</li> </ol> <pre><code>docker-compose build\n</code></pre> <ol> <li>Start the stack:</li> </ol> <pre><code>docker-compose up -d\n</code></pre> <ol> <li>Verify services:</li> </ol> <pre><code>docker-compose ps\n</code></pre> <ol> <li> <p>Access dashboards:</p> </li> <li> <p>Grafana: http://localhost:3000</p> </li> <li>Prometheus: http://localhost:9090</li> <li>Loki: http://localhost:3100</li> </ol>"},{"location":"deployment/docker/#notes","title":"Notes","text":"<ul> <li>Each producer requires its own Selenium driver instance. Docker Compose sets up individual containers with separate drivers.</li> <li>Work queue initialization is handled by the primary producer.</li> <li>DLQ messages can be inspected via RabbitMQ management UI or by custom ETL scripts.</li> <li>Logs are forwarded via Promtail \u2192 Loki for centralized querying.</li> </ul>"},{"location":"deployment/k8s/","title":"Kubernetes Deployment","text":"<p>This guide describes deploying the BBC News ETL pipeline in production-like Kubernetes environments, including KEDA autoscaling.</p>"},{"location":"deployment/k8s/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (Minikube, Kind, or cloud provider)</li> <li>Helm &gt;= 3.x</li> <li>kubectl CLI</li> <li>Optional: KEDA for autoscaling</li> </ul>"},{"location":"deployment/k8s/#architecture","title":"Architecture","text":"<ul> <li>Primary Producer \u2192 initializes work queue</li> <li>Multiple Producers \u2192 scrape articles, deduplicate, publish tasks</li> <li>RabbitMQ \u2192 message broker for Task Queue &amp; DLQ</li> <li>Consumers (ETL Workers) \u2192 fetch and process tasks</li> <li>MongoDB / PostgreSQL \u2192 data storage</li> <li>Prometheus / Grafana / Loki + Promtail \u2192 observability</li> <li>KEDA \u2192 scales producers/consumers based on queue length</li> </ul>"},{"location":"deployment/k8s/#deployment-steps","title":"Deployment Steps","text":"<ol> <li>Clone the repo:</li> </ol> <pre><code>git clone https://github.com/Rahul-404/bbc_news_etl_pipeline.git\ncd bbc_news_etl_pipeline\n</code></pre> <ol> <li>Install RabbitMQ, MongoDB, PostgreSQL, Prometheus, Grafana, Loki using Helm charts:</li> </ol> <pre><code>helm install rabbitmq ./helm/rabbitmq\nhelm install mongo ./helm/mongodb\nhelm install postgres ./helm/postgres\nhelm install prometheus ./helm/prometheus\nhelm install grafana ./helm/grafana\nhelm install loki ./helm/loki\n</code></pre> <ol> <li>Deploy Primary Producer, Producers, and Consumers:</li> </ol> <pre><code>kubectl apply -f k8s/producers/\nkubectl apply -f k8s/consumers/\n</code></pre> <ol> <li> <p>Configure KEDA for horizontal scaling:</p> </li> <li> <p>Producers scale based on Work Queue length.</p> </li> <li>Consumers scale based on Task Queue depth.</li> <li> <p>Example KEDA ScaledObject YAML is included in <code>k8s/keda/</code>.</p> </li> <li> <p>Verify Pods and Services:</p> </li> </ol> <pre><code>kubectl get pods\nkubectl get svc\n</code></pre> <ol> <li> <p>Access dashboards:</p> </li> <li> <p>Grafana: http://:3000 <li>Prometheus: http://:9090 <li>Loki: http://:3100"},{"location":"deployment/k8s/#notes","title":"Notes","text":"<ul> <li>Each Producer pod contains its own Selenium driver for scraping.</li> <li>DLQ handling is automatic: failed ETL messages remain in RabbitMQ for manual inspection.</li> <li>Logging and metrics are fully integrated, ready for production-grade monitoring.</li> <li>Helm charts allow environment-specific configurations via <code>values.yaml</code>.</li> </ul>"},{"location":"development/layout/","title":"Project Layout","text":"<p>This guide covers how to deploy the BBC News Scraper ETL Pipeline in two ways:</p> <ul> <li>Local deployment (for development and testing)</li> <li>Cloud deployment (for staging or production environments)</li> </ul>"},{"location":"development/layout/#1-local-deployment-docker-kubernetes-on-docker-desktop","title":"1. Local Deployment (Docker + Kubernetes on Docker Desktop)","text":""},{"location":"development/layout/#overview","title":"Overview","text":"<p>Local deployment is intended for development and testing. All services (databases, queues, monitoring) run as Docker containers, and the ETL pipeline runs inside a local Kubernetes cluster.</p>"},{"location":"development/layout/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop (with Kubernetes enabled)</li> <li>Terraform</li> <li>Helm</li> <li>kubectl</li> <li>git</li> </ul>"},{"location":"development/layout/#components","title":"Components","text":"Component Tool Deployment Method MongoDB Docker <code>docker-compose</code> PostgreSQL Docker <code>docker-compose</code> RabbitMQ Docker <code>docker-compose</code> Prometheus Docker <code>docker-compose</code> Loki Docker <code>docker-compose</code> Grafana Docker <code>docker-compose</code> ETL Jobs Kubernetes Terraform / YAML Promtail Kubernetes Terraform / YAML"},{"location":"development/layout/#folder-structure-recommended","title":"Folder Structure (recommended)","text":"<pre><code>/deploy/local/\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 terraform/\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2514\u2500\u2500 variables.tf\n\u2514\u2500\u2500 k8s/\n    \u251c\u2500\u2500 worker-job.yaml\n    \u251c\u2500\u2500 assigner-deployment.yaml\n    \u2514\u2500\u2500 promtail.yaml\n</code></pre>"},{"location":"development/layout/#steps","title":"Steps","text":"<ol> <li>Start infrastructure services:</li> </ol> <pre><code>docker-compose up -d\n</code></pre> <ol> <li>Deploy Kubernetes resources:</li> </ol> <pre><code>cd deploy/local/terraform\nterraform init\nterraform apply\n</code></pre> <ol> <li> <p>Verify:</p> </li> <li> <p>Grafana: http://localhost:3000</p> </li> <li>RabbitMQ: http://localhost:15672</li> <li>ETL logs via Grafana (Loki)</li> </ol>"},{"location":"development/layout/#2-cloud-deployment-aws","title":"2. Cloud Deployment (AWS)","text":""},{"location":"development/layout/#overview_1","title":"Overview","text":"<p>Cloud deployment is designed for production or staging environments. The same architecture is deployed on AWS using managed services and Terraform for infrastructure.</p>"},{"location":"development/layout/#services-used","title":"Services Used","text":"Component AWS Equivalent MongoDB MongoDB Atlas / self-hosted on EC2 PostgreSQL Amazon RDS RabbitMQ Amazon MQ / self-hosted Kubernetes Amazon EKS Logs AWS CloudWatch / Loki Metrics Amazon Managed Prometheus / self-hosted Dashboard Grafana Cloud / EC2"},{"location":"development/layout/#tools-required","title":"Tools Required","text":"<ul> <li>Terraform CLI</li> <li>AWS CLI (configured with credentials)</li> <li>kubectl</li> <li>Helm</li> </ul>"},{"location":"development/layout/#folder-structure-recommended_1","title":"Folder Structure (recommended)","text":"<pre><code>/deploy/aws/\n\u251c\u2500\u2500 terraform/\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 eks-cluster.tf\n\u2502   \u251c\u2500\u2500 rds.tf\n\u2502   \u2514\u2500\u2500 outputs.tf\n\u2514\u2500\u2500 k8s/\n    \u251c\u2500\u2500 worker-job.yaml\n    \u251c\u2500\u2500 assigner-deployment.yaml\n    \u2514\u2500\u2500 promtail.yaml\n</code></pre>"},{"location":"development/layout/#steps_1","title":"Steps","text":"<ol> <li>Provision infrastructure with Terraform:</li> </ol> <pre><code>cd deploy/aws/terraform\nterraform init\nterraform apply\n</code></pre> <ol> <li>Configure kubectl:</li> </ol> <pre><code>aws eks update-kubeconfig --name my-cluster\n</code></pre> <ol> <li>Deploy ETL components to EKS:</li> </ol> <pre><code>kubectl apply -f ../k8s/\n</code></pre> <ol> <li> <p>Access Monitoring Dashboards:</p> </li> <li> <p>Grafana Cloud or EC2 URL</p> </li> <li>AWS CloudWatch (if used for logs/metrics)</li> </ol>"},{"location":"development/testing/","title":"Running Tests &amp; Pre-commit","text":"<p>Testing ensures code quality and reliability. This project uses both unit and integration tests.</p>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":"<p>Use <code>pytest</code> to run tests. You can run all tests or target specific types:</p>"},{"location":"development/testing/#run-all-tests","title":"Run All Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"development/testing/#run-only-unit-tests","title":"Run Only Unit Tests","text":"<pre><code>pytest tests/unit/\n</code></pre>"},{"location":"development/testing/#run-only-integration-tests","title":"Run Only Integration Tests","text":"<pre><code>pytest tests/integration/\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":"<ul> <li>All tests should be placed in the <code>tests/</code> directory.</li> <li>Test files should be named using the pattern: <code>test_*.py</code></li> </ul>"},{"location":"development/testing/#directory-structure","title":"Directory Structure","text":"<pre><code>tests/\n\u2502\n\u251c\u2500\u2500 unit/             # Unit tests\n\u2502   \u2514\u2500\u2500 test_*.py\n\u2502\n\u251c\u2500\u2500 integration/      # Integration tests\n\u2502   \u2514\u2500\u2500 test_*.py\n</code></pre>"},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<ul> <li>Test individual functions or classes in isolation.</li> <li>Use mocking as needed to isolate components.</li> <li>Fast to run and should not rely on external systems (e.g., databases, APIs).</li> </ul>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<ul> <li>Test interactions between multiple components or services.</li> <li>Use Docker Compose to spin up required services (e.g., databases, message queues).</li> <li>Tests run against these real service instances to validate integration.</li> <li>Slower than unit tests but ensure end-to-end component cooperation.</li> </ul> Warning <p>Integration tests depend on services started via Docker Compose.</p> <p>Before running integration tests, ensure Docker and Docker Compose are installed and running on your machine.</p> <p>Also, start required services with <code>docker-compose up -d</code> and stop them with <code>docker-compose down</code> after testing.</p> <p>Failing to do so may result in test failures or inconsistent test results.</p>"},{"location":"development/testing/#running-integration-tests-with-docker-compose","title":"Running Integration Tests with Docker Compose","text":"<ol> <li>Define your services in a <code>docker-compose.yml</code> file at the project root. For example:</li> </ol> <pre><code>version: '3.8'\nservices:\n  db:\n    image: postgres:13\n    environment:\n      POSTGRES_USER: testuser\n      POSTGRES_PASSWORD: testpass\n      POSTGRES_DB: testdb\n    ports:\n      - \"5432:5432\"\n</code></pre> <ol> <li>Start the services before running integration tests:</li> </ol> <pre><code>docker-compose up -d\n</code></pre> <ol> <li>Run integration tests:</li> </ol> <pre><code>pytest tests/integration/\n</code></pre> <ol> <li>After tests finish, stop and remove services:</li> </ol> <pre><code>docker-compose down\n</code></pre>"},{"location":"development/testing/#using-fixtures","title":"Using Fixtures","text":"<ul> <li>Use <code>pytest</code> fixtures to set up and tear down connections to services started by Docker Compose.</li> <li>You can also automate Docker Compose lifecycle within fixtures using Python subprocess calls or specialized libraries like <code>pytest-docker-compose</code>.</li> </ul>"},{"location":"getting_started/k8s_deployment/","title":"Kubernetes Deployment (Helm/Kind)","text":""},{"location":"getting_started/k8s_deployment/#1-running-on-kubernetes-kind-helm","title":"1. Running on Kubernetes (Kind / Helm)","text":"<p>For a production-like deployment with scalability:</p> <ol> <li>Create a Kubernetes cluster (Kind example):</li> </ol> <pre><code>kind create cluster --name bbc-news-etl\n</code></pre> <ol> <li>Deploy Helm charts:</li> </ol> <pre><code>helm install bbc-news-etl ./helm/bbc-news-etl\n</code></pre> <ol> <li>Check pods and services:</li> </ol> <pre><code>kubectl get pods\nkubectl get svc\n</code></pre> <ol> <li>Access Grafana (for monitoring):</li> </ol> <pre><code>kubectl port-forward svc/grafana 3000:3000\n</code></pre>"},{"location":"getting_started/k8s_deployment/#2-next-steps","title":"2. Next Steps","text":"<ul> <li>Explore architecture/overview.md for system design.</li> <li>Dive into components/ for detailed ETL workflow.</li> <li>Check configuration/ for environment variables and setup.</li> </ul>"},{"location":"getting_started/local_setup/","title":"Local Setup (Docker Compose)","text":""},{"location":"getting_started/local_setup/#1-running-locally-with-docker-compose","title":"1. Running Locally with Docker Compose","text":"<p>This method runs all services (Producer, Consumer, RabbitMQ, MongoDB, PostgreSQL) in containers for quick testing.</p> <ol> <li>Build Docker images:</li> </ol> <pre><code>docker compose build\n</code></pre> <ol> <li>Start the services:</li> </ol> <pre><code>docker compose up -d\n</code></pre> <ol> <li>Verify services are running:</li> </ol> <pre><code>docker compose ps\n</code></pre> <ol> <li>Stop services when done:</li> </ol> <pre><code>docker compose down\n</code></pre>"},{"location":"getting_started/local_setup/#2-run-tests-pre-commit-hooks","title":"2. Run Tests &amp; Pre-commit Hooks","text":"<p>Ensure code quality and reliability:</p> <pre><code># Run unit tests\npytest tests/\n\n# Run pre-commit checks\npre-commit run --all-files\n</code></pre>"},{"location":"getting_started/local_setup/#3-next-steps","title":"3. Next Steps","text":"<ul> <li>Explore architecture/overview.md for system design.</li> <li>Dive into components/ for detailed ETL workflow.</li> <li>Check configuration/ for environment variables and setup.</li> </ul>"},{"location":"getting_started/prerequisites/","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.11+</li> <li>Docker and Docker Compose</li> <li>Kubernetes (Kind or Minikube)</li> <li>Helm 3+</li> <li>Git</li> </ul> <p>Optional (for monitoring and observability):</p> <ul> <li>Prometheus</li> <li>Grafana</li> <li>Loki</li> </ul> <p>Clone the repository:</p> <pre><code>git clone https://github.com/Rahul-404/bbc_news_etl_pipeline.git\ncd bbc_news_etl_pipeline\n</code></pre>"},{"location":"observability/dashboards/","title":"Dashboards (Grafana)","text":"<p>Grafana provides real-time visualizations of the BBC News ETL pipeline metrics collected by Prometheus. Dashboards allow developers and operators to quickly assess pipeline health, throughput, and failures.</p>"},{"location":"observability/dashboards/#1-overview","title":"1. Overview","text":"<ul> <li>Grafana connects to Prometheus as the data source.</li> <li>Provides custom dashboards for producers, consumers, RabbitMQ, and overall pipeline performance.</li> <li>Dashboards are pre-configured via JSON files or Helm charts.</li> <li>Supports alerts and notifications (Slack, email, etc.) for critical conditions.</li> </ul>"},{"location":"observability/dashboards/#2-example-dashboards","title":"2. Example Dashboards","text":"Dashboard Description Producer Overview Shows articles scraped, work queue length, scrape failures, and retries by section. Consumer / ETL Dashboard Displays messages consumed, ETL success/failure counts, retry attempts, and DLQ messages. RabbitMQ Queue Status Queue depth, message rate, DLQ count, consumer count. Pipeline Health Combined metrics for producers, consumers, and RabbitMQ, including scaling events (KEDA)."},{"location":"observability/dashboards/#3-integration","title":"3. Integration","text":"<ul> <li>Grafana is included as a Docker container for local development or deployed via Helm chart in Kubernetes.</li> <li>Dashboards can be imported/exported using JSON.</li> <li>Use templating variables to switch between sections, queues, or environments dynamically.</li> </ul>"},{"location":"observability/dashboards/#4-best-practices","title":"4. Best Practices","text":"<ul> <li>Keep critical metrics at the top for quick visibility.</li> <li>Configure alerts for DLQ accumulation, work queue backlog, or high ETL failure rate.</li> <li>Use annotations to mark deployments or incidents on dashboards.</li> <li>Combine logs and metrics for root-cause analysis.</li> </ul>"},{"location":"observability/logs/","title":"Logs (Loki + Promtail)","text":"<p>Loki provides centralized logging for all components of the BBC News ETL pipeline. Promtail agents collect logs from each container/pod and send them to Loki for querying and visualization.</p>"},{"location":"observability/logs/#1-overview","title":"1. Overview","text":"<ul> <li>Logs include errors, retries, debug info, and operational events from producers, consumers, and RabbitMQ.</li> <li>Loki integrates with Grafana to visualize logs alongside metrics.</li> <li>Promtail runs as a sidecar or standalone agent to collect container logs.</li> </ul>"},{"location":"observability/logs/#2-log-sources","title":"2. Log Sources","text":"Component Logs Producers Scrape start/end, articles processed, retries, Selenium errors, task queue updates Consumers ETL start/end, success/failure counts, DLQ messages, database errors RabbitMQ Message publish/consume events, queue depth alerts System / Kubernetes Pod lifecycle events, scaling actions, resource metrics"},{"location":"observability/logs/#3-log-format","title":"3. Log Format","text":"<ul> <li>Structured logs in JSON are recommended for better querying.</li> <li>Example log entry from a producer:</li> </ul> <pre><code>{\n  \"timestamp\": \"2025-09-28T12:00:00Z\",\n  \"level\": \"INFO\",\n  \"component\": \"producer\",\n  \"section\": \"world\",\n  \"articles_scraped\": 15,\n  \"task_queue_length\": 12\n}\n</code></pre> <ul> <li>Example log entry from a consumer:</li> </ul> <pre><code>{\n  \"timestamp\": \"2025-09-28T12:01:00Z\",\n  \"level\": \"ERROR\",\n  \"component\": \"consumer\",\n  \"etl_task_id\": \"abc123\",\n  \"error\": \"PostgreSQL connection timeout\",\n  \"retry_count\": 2\n}\n</code></pre>"},{"location":"observability/logs/#4-integration","title":"4. Integration","text":"<ul> <li>Promtail reads container stdout/stderr, files, or systemd logs.</li> <li>Sends logs to Loki over HTTP or gRPC endpoints.</li> <li>Grafana dashboards can correlate logs with metrics to identify pipeline bottlenecks.</li> </ul>"},{"location":"observability/logs/#5-best-practices","title":"5. Best Practices","text":"<ul> <li>Label logs with <code>component</code>, <code>section</code>, <code>task_id</code>, and <code>environment</code> for filtering.</li> <li>Monitor DLQ-related logs to ensure no messages are lost.</li> <li>Use retention policies to manage storage.</li> <li>Secure Loki endpoints and use role-based access control for production environments.</li> </ul>"},{"location":"observability/metrics/","title":"Metrics (Prometheus)","text":"<p>The BBC News ETL Pipeline exposes rich metrics via Prometheus to provide real-time insights into pipeline performance, queue health, processing rates, and failures.</p>"},{"location":"observability/metrics/#1-overview","title":"1. Overview","text":"<p>Prometheus collects metrics from producers, consumers, and RabbitMQ. Metrics allow:</p> <ul> <li>Monitoring throughput and latency.</li> <li>Tracking queue depths and backlogs.</li> <li>Alerting on failures, DLQ accumulation, or resource saturation.</li> <li>Observing scaling behavior when KEDA adjusts the number of producers/consumers.</li> </ul>"},{"location":"observability/metrics/#2-metrics-sources","title":"2. Metrics Sources","text":"Component Metrics Type Description Producers Counter / Gauge Number of article links scraped, success/failure counts, active tasks, work queue size Consumers (ETL Workers) Counter / Gauge Messages consumed, ETL success/failure counts, retry attempts, DLQ counts RabbitMQ Queue metrics Task queue depth, DLQ depth, message publish/consume rate, queue latency Kubernetes / KEDA Custom metrics Pod replicas, CPU/Memory usage, scaling events"},{"location":"observability/metrics/#3-example-metrics","title":"3. Example Metrics","text":""},{"location":"observability/metrics/#producer-metrics","title":"Producer Metrics","text":"<pre><code>bbc_producer_articles_scraped_total{section=\"world\"} 1234\nbbc_producer_scrape_failures_total{section=\"tech\"} 12\nbbc_producer_workqueue_length 15\n</code></pre>"},{"location":"observability/metrics/#consumer-metrics","title":"Consumer Metrics","text":"<pre><code>bbc_consumer_messages_processed_total 4567\nbbc_consumer_etl_failures_total 34\nbbc_consumer_dlq_messages_total 5\n</code></pre>"},{"location":"observability/metrics/#rabbitmq-metrics","title":"RabbitMQ Metrics","text":"<pre><code>rabbitmq_queue_messages{queue=\"task_queue\"} 120\nrabbitmq_queue_messages{queue=\"dlq_queue\"} 3\nrabbitmq_queue_consumers 4\n</code></pre>"},{"location":"observability/metrics/#4-integration","title":"4. Integration","text":"<ul> <li>Prometheus Exporter is embedded in each component (Python <code>prometheus_client</code>).</li> <li>Metrics are scraped automatically by the Prometheus server.</li> <li>Producers, Consumers, and RabbitMQ expose <code>/metrics</code> endpoints.</li> </ul> <p>Example:</p> <pre><code>from prometheus_client import start_http_server, Counter\n\narticles_scraped = Counter('bbc_producer_articles_scraped_total', 'Total articles scraped')\nstart_http_server(8000)  # exposes /metrics\n</code></pre> <ul> <li>Prometheus pulls metrics at a configurable interval (default: 15s).</li> </ul>"},{"location":"observability/metrics/#5-best-practices","title":"5. Best Practices","text":"<ul> <li>Use meaningful labels (e.g., <code>section</code>, <code>status</code>, <code>queue</code>) for filtering in Grafana.</li> <li> <p>Set alerts for:</p> </li> <li> <p>Work queue backlog &gt; threshold</p> </li> <li>DLQ accumulation &gt; threshold</li> <li>ETL failure rate &gt; threshold</li> <li>Combine Prometheus metrics with Loki logs for full observability.</li> <li>Ensure metrics endpoints are secured in production deployments.</li> </ul>"},{"location":"scaling/","title":"Scaling &amp; Reliability","text":"<p>The BBC News ETL Pipeline is designed to handle large-scale news ingestion while maintaining high reliability. This section explains how the system achieves horizontal scaling, fault tolerance, and message reliability.</p>"},{"location":"scaling/#1-horizontal-scaling","title":"1. Horizontal Scaling","text":"<p>The pipeline supports dynamic scaling of producers and consumers based on workload:</p>"},{"location":"scaling/#producers-scrapers","title":"Producers (Scrapers)","text":"<ul> <li>Multiple producer instances can run in parallel to scrape different sections or dates.</li> <li>Work queue ensures each producer picks unique tasks to avoid duplication.</li> <li>KEDA autoscaling triggers scale-up or scale-down based on work queue length.</li> </ul>"},{"location":"scaling/#consumers-etl-workers","title":"Consumers (ETL Workers)","text":"<ul> <li>Multiple consumers process tasks from RabbitMQ concurrently.</li> <li>Scaling is triggered based on Task Queue depth.</li> <li>Allows handling spikes in incoming article links efficiently.</li> </ul>"},{"location":"scaling/#2-keda-integration","title":"2. KEDA Integration","text":"<ul> <li>KEDA (Kubernetes Event-driven Autoscaling) is used to dynamically scale pods.</li> <li>Primary producer builds a work queue for dates requiring scraping.</li> <li>KEDA monitors the queue and spins up multiple producers as needed.</li> <li>Similarly, KEDA scales consumers based on Task Queue length.</li> <li>Ensures optimal resource usage without over-provisioning.</li> </ul>"},{"location":"scaling/#3-reliability-fault-tolerance","title":"3. Reliability &amp; Fault Tolerance","text":""},{"location":"scaling/#message-delivery","title":"Message Delivery","text":"<ul> <li>RabbitMQ provides durable queues with acknowledgements to ensure no message loss.</li> <li>Failed ETL messages are sent to the Dead Letter Queue (DLQ) for manual inspection or retry.</li> <li>Retry policies and exponential backoff are applied before sending messages to DLQ.</li> </ul>"},{"location":"scaling/#data-reliability","title":"Data Reliability","text":"<ul> <li>MongoDB stores raw HTML for archival purposes.</li> <li>PostgreSQL stores cleaned, analytics-ready datasets.</li> <li>Versioned datasets support reproducibility of ETL pipelines.</li> </ul>"},{"location":"scaling/#error-handling","title":"Error Handling","text":"<ul> <li>Producers handle rate limits, retries, and Selenium errors gracefully.</li> <li>Consumers log errors and failures to Loki, while metrics are sent to Prometheus for alerting.</li> <li>Observability allows operators to detect bottlenecks or failures early.</li> </ul>"},{"location":"scaling/#4-key-highlights","title":"4. Key Highlights","text":"<ul> <li>Dynamic Scaling: Producers &amp; Consumers scale automatically with KEDA.</li> <li>High Availability: Message queues, storage, and observability tools are resilient.</li> <li>Retry &amp; DLQ: Ensures no data loss and reliable message processing.</li> <li>Resource Efficiency: Kubernetes orchestrates pods efficiently, scaling only when needed.</li> </ul>"},{"location":"scaling/#5-scaling-diagram","title":"5. Scaling Diagram","text":"<pre><code>flowchart TD\n    WorkQueue[\"Work Queue (Dates)\"] --&gt; KEDAProducers[\"KEDA: Scale Producers\"]\n    KEDAProducers --&gt; Producers[\"Multiple Producers\"]\n    Producers --&gt; TaskQueue[\"Task Queue (Article Links)\"]\n    TaskQueue --&gt; KEDAConsumers[\"KEDA: Scale Consumers\"]\n    KEDAConsumers --&gt; Consumers[\"Consumers (ETL Workers)\"]\n    Consumers --&gt; MongoDB[\"Raw Storage\"]\n    Consumers --&gt; Postgres[\"Clean Storage\"]\n    Consumers --&gt; DLQ[\"Dead Letter Queue\"]\n    TaskQueue --&gt; Metrics[\"Prometheus Metrics\"]\n    Producers --&gt; Logs[\"Loki Logs\"]\n    Consumers --&gt; Logs</code></pre>"}]}